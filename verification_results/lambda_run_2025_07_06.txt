import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=0.12).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128),
                                    use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.22), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
for _ in range(warmup_steps):
    inputs = targets = torch.randint(0, args.vocab_size, size=(args.train_seq_len,), device="cuda")
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

########################################
#      Overlap Communication Setup     #
########################################

# Create parameter buckets for better overlap
def create_buckets(params, bucket_size_mb=25):
    """Group parameters into buckets of approximately bucket_size_mb MB each"""
    buckets = []
    current_bucket = []
    current_size = 0

    # Sort parameters by size (largest first) for better bucketing
    sorted_params = sorted(params, key=lambda p: p.numel(), reverse=True)

    for param in sorted_params:
        param_size_mb = param.numel() * param.element_size() / (1024 * 1024)

        if current_size + param_size_mb > bucket_size_mb and current_bucket:
            buckets.append(current_bucket)
            current_bucket = [param]
            current_size = param_size_mb
        else:
            current_bucket.append(param)
            current_size += param_size_mb

    if current_bucket:
        buckets.append(current_bucket)

    return buckets

# Create buckets for all parameters
all_params = [p for p in model.parameters() if p.requires_grad]
param_buckets = create_buckets(all_params)

print0(f"Created {len(param_buckets)} gradient buckets")
for i, bucket in enumerate(param_buckets):
    total_size = sum(p.numel() * p.element_size() for p in bucket) / (1024 * 1024)
    print0(f"Bucket {i}: {len(bucket)} params, {total_size:.1f} MB")

# Bucket state tracking
bucket_ready_count = [0] * len(param_buckets)
bucket_handles = [None] * len(param_buckets)
param_to_bucket = {}

# Map each parameter to its bucket index
for bucket_idx, bucket in enumerate(param_buckets):
    for param in bucket:
        param_to_bucket[param] = bucket_idx

def _gradient_hook(param: Tensor):
    """Called when a parameter's gradient is ready"""
    if param.grad is None:
        return

    bucket_idx = param_to_bucket[param]
    bucket_ready_count[bucket_idx] += 1

    # Check if all parameters in this bucket are ready
    if bucket_ready_count[bucket_idx] == len(param_buckets[bucket_idx]):
        # All-reduce this bucket
        bucket_grads = [p.grad for p in param_buckets[bucket_idx]]

        # For multi-tensor operations, we can reduce them together
        if len(bucket_grads) == 1:
            handle = dist.all_reduce(bucket_grads[0], op=dist.ReduceOp.AVG, async_op=True)
        else:
            # Use multi-tensor all-reduce for efficiency
            handle = dist.all_reduce_coalesced(bucket_grads, op=dist.ReduceOp.AVG, async_op=True)

        bucket_handles[bucket_idx] = handle

# Register hooks for all parameters
print0("Registering bucketed gradient hooks...")
for param in all_params:
    param.register_post_accumulate_grad_hook(_gradient_hook)

def wait_for_gradients():
    """Wait for all gradient reductions to complete and reset bucket state"""
    for handle in bucket_handles:
        if handle is not None:
            handle.wait()

    # Reset state for next iteration
    for i in range(len(bucket_ready_count)):
        bucket_ready_count[i] = 0
        bucket_handles[i] = None

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    #for param in model.parameters():
    #    dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    wait_for_gradients() # does the same thing as commented two lines above, but faster

    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250706+cu126 compiled for CUDA 12.6
Sun Jul  6 21:10:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   34C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   33C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   31C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   30C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            5445      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A            5446      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A            5447      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A            5448      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A            5449      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A            5450      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A            5451      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A            5452      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A            5446      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A            5447      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A            5448      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A            5449      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A            5450      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A            5451      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A            5452      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Created 22 gradient buckets
Bucket 0: 1 params, 147.4 MB
Bucket 1: 1 params, 73.6 MB
Bucket 2: 1 params, 73.6 MB
Bucket 3: 1 params, 73.6 MB
Bucket 4: 1 params, 73.6 MB
Bucket 5: 2 params, 18.0 MB
Bucket 6: 2 params, 18.0 MB
Bucket 7: 2 params, 18.0 MB
Bucket 8: 2 params, 18.0 MB
Bucket 9: 2 params, 18.0 MB
Bucket 10: 2 params, 18.0 MB
Bucket 11: 2 params, 18.0 MB
Bucket 12: 2 params, 18.0 MB
Bucket 13: 2 params, 18.0 MB
Bucket 14: 2 params, 18.0 MB
Bucket 15: 2 params, 18.0 MB
Bucket 16: 3 params, 24.8 MB
Bucket 17: 3 params, 20.2 MB
Bucket 18: 3 params, 20.2 MB
Bucket 19: 3 params, 20.2 MB
Bucket 20: 9 params, 24.8 MB
Bucket 21: 27 params, 6.8 MB
Registering bucketed gradient hooks...
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1770 train_time:536ms step_avg:535.66ms
step:2/1770 train_time:564ms step_avg:281.98ms
step:3/1770 train_time:652ms step_avg:217.48ms
step:4/1770 train_time:745ms step_avg:186.27ms
step:5/1770 train_time:839ms step_avg:167.78ms
step:6/1770 train_time:933ms step_avg:155.51ms
step:7/1770 train_time:1027ms step_avg:146.65ms
step:8/1770 train_time:1121ms step_avg:140.08ms
step:9/1770 train_time:1215ms step_avg:135.01ms
step:10/1770 train_time:1309ms step_avg:130.87ms
step:11/1770 train_time:1402ms step_avg:127.47ms
step:12/1770 train_time:1503ms step_avg:125.21ms
step:13/1770 train_time:1605ms step_avg:123.47ms
step:14/1770 train_time:1702ms step_avg:121.58ms
step:15/1770 train_time:1796ms step_avg:119.72ms
step:16/1770 train_time:1889ms step_avg:118.09ms
step:17/1770 train_time:1983ms step_avg:116.67ms
step:18/1770 train_time:2077ms step_avg:115.38ms
step:19/1770 train_time:2171ms step_avg:114.25ms
step:20/1770 train_time:2265ms step_avg:113.23ms
step:21/1770 train_time:2359ms step_avg:112.33ms
step:22/1770 train_time:2455ms step_avg:111.57ms
step:23/1770 train_time:2552ms step_avg:110.94ms
step:24/1770 train_time:2648ms step_avg:110.35ms
step:25/1770 train_time:2744ms step_avg:109.77ms
step:26/1770 train_time:2838ms step_avg:109.16ms
step:27/1770 train_time:2932ms step_avg:108.60ms
step:28/1770 train_time:3027ms step_avg:108.09ms
step:29/1770 train_time:3121ms step_avg:107.63ms
step:30/1770 train_time:3215ms step_avg:107.17ms
step:31/1770 train_time:3309ms step_avg:106.74ms
step:32/1770 train_time:3403ms step_avg:106.36ms
step:33/1770 train_time:3499ms step_avg:106.04ms
step:34/1770 train_time:3595ms step_avg:105.74ms
step:35/1770 train_time:3690ms step_avg:105.42ms
step:36/1770 train_time:3785ms step_avg:105.15ms
step:37/1770 train_time:3880ms step_avg:104.87ms
step:38/1770 train_time:3975ms step_avg:104.59ms
step:39/1770 train_time:4069ms step_avg:104.33ms
step:40/1770 train_time:4163ms step_avg:104.07ms
step:41/1770 train_time:4257ms step_avg:103.83ms
step:42/1770 train_time:4352ms step_avg:103.61ms
step:43/1770 train_time:4446ms step_avg:103.39ms
step:44/1770 train_time:4542ms step_avg:103.23ms
step:45/1770 train_time:4639ms step_avg:103.08ms
step:46/1770 train_time:4733ms step_avg:102.90ms
step:47/1770 train_time:4828ms step_avg:102.72ms
step:48/1770 train_time:4923ms step_avg:102.56ms
step:49/1770 train_time:5017ms step_avg:102.39ms
step:50/1770 train_time:5111ms step_avg:102.22ms
step:51/1770 train_time:5206ms step_avg:102.08ms
step:52/1770 train_time:5301ms step_avg:101.94ms
step:53/1770 train_time:5395ms step_avg:101.80ms
step:54/1770 train_time:5489ms step_avg:101.65ms
step:55/1770 train_time:5584ms step_avg:101.52ms
step:56/1770 train_time:5679ms step_avg:101.41ms
step:57/1770 train_time:5775ms step_avg:101.32ms
step:58/1770 train_time:5870ms step_avg:101.20ms
step:59/1770 train_time:5964ms step_avg:101.09ms
step:60/1770 train_time:6058ms step_avg:100.96ms
step:61/1770 train_time:6152ms step_avg:100.85ms
step:62/1770 train_time:6247ms step_avg:100.76ms
step:63/1770 train_time:6342ms step_avg:100.67ms
step:64/1770 train_time:6436ms step_avg:100.57ms
step:65/1770 train_time:6531ms step_avg:100.47ms
step:66/1770 train_time:6627ms step_avg:100.41ms
step:67/1770 train_time:6722ms step_avg:100.33ms
step:68/1770 train_time:6817ms step_avg:100.25ms
step:69/1770 train_time:6912ms step_avg:100.17ms
step:70/1770 train_time:7006ms step_avg:100.09ms
step:71/1770 train_time:7101ms step_avg:100.02ms
step:72/1770 train_time:7196ms step_avg:99.94ms
step:73/1770 train_time:7290ms step_avg:99.86ms
step:74/1770 train_time:7384ms step_avg:99.78ms
step:75/1770 train_time:7478ms step_avg:99.71ms
step:76/1770 train_time:7573ms step_avg:99.64ms
step:77/1770 train_time:7669ms step_avg:99.59ms
step:78/1770 train_time:7764ms step_avg:99.54ms
step:79/1770 train_time:7859ms step_avg:99.48ms
step:80/1770 train_time:7953ms step_avg:99.41ms
step:81/1770 train_time:8047ms step_avg:99.35ms
step:82/1770 train_time:8144ms step_avg:99.31ms
step:83/1770 train_time:8238ms step_avg:99.25ms
step:84/1770 train_time:8332ms step_avg:99.19ms
step:85/1770 train_time:8427ms step_avg:99.15ms
step:86/1770 train_time:8522ms step_avg:99.09ms
step:87/1770 train_time:8617ms step_avg:99.04ms
step:88/1770 train_time:8711ms step_avg:98.99ms
step:89/1770 train_time:8805ms step_avg:98.93ms
step:90/1770 train_time:8899ms step_avg:98.88ms
step:91/1770 train_time:8994ms step_avg:98.83ms
step:92/1770 train_time:9088ms step_avg:98.78ms
step:93/1770 train_time:9181ms step_avg:98.73ms
step:94/1770 train_time:9276ms step_avg:98.68ms
step:95/1770 train_time:9370ms step_avg:98.63ms
step:96/1770 train_time:9464ms step_avg:98.59ms
step:97/1770 train_time:9559ms step_avg:98.54ms
step:98/1770 train_time:9654ms step_avg:98.51ms
step:99/1770 train_time:9749ms step_avg:98.47ms
step:100/1770 train_time:9843ms step_avg:98.43ms
step:101/1770 train_time:9938ms step_avg:98.40ms
step:102/1770 train_time:10033ms step_avg:98.36ms
step:103/1770 train_time:10129ms step_avg:98.34ms
step:104/1770 train_time:10224ms step_avg:98.31ms
step:105/1770 train_time:10319ms step_avg:98.28ms
step:106/1770 train_time:10413ms step_avg:98.24ms
step:107/1770 train_time:10507ms step_avg:98.20ms
step:108/1770 train_time:10602ms step_avg:98.16ms
step:109/1770 train_time:10697ms step_avg:98.14ms
step:110/1770 train_time:10791ms step_avg:98.10ms
step:111/1770 train_time:10886ms step_avg:98.07ms
step:112/1770 train_time:10980ms step_avg:98.04ms
step:113/1770 train_time:11076ms step_avg:98.01ms
step:114/1770 train_time:11170ms step_avg:97.98ms
step:115/1770 train_time:11265ms step_avg:97.96ms
step:116/1770 train_time:11360ms step_avg:97.93ms
step:117/1770 train_time:11455ms step_avg:97.90ms
step:118/1770 train_time:11550ms step_avg:97.88ms
step:119/1770 train_time:11645ms step_avg:97.86ms
step:120/1770 train_time:11739ms step_avg:97.83ms
step:121/1770 train_time:11834ms step_avg:97.80ms
step:122/1770 train_time:11929ms step_avg:97.78ms
step:123/1770 train_time:12024ms step_avg:97.75ms
step:124/1770 train_time:12118ms step_avg:97.73ms
step:125/1770 train_time:12212ms step_avg:97.70ms
step:125/1770 val_loss:4.6363 train_time:12307ms step_avg:98.46ms
step:126/1770 train_time:12326ms step_avg:97.82ms
step:127/1770 train_time:12410ms step_avg:97.71ms
step:128/1770 train_time:12514ms step_avg:97.77ms
step:129/1770 train_time:12611ms step_avg:97.76ms
step:130/1770 train_time:12706ms step_avg:97.74ms
step:131/1770 train_time:12799ms step_avg:97.71ms
step:132/1770 train_time:12893ms step_avg:97.68ms
step:133/1770 train_time:12988ms step_avg:97.65ms
step:134/1770 train_time:13082ms step_avg:97.63ms
step:135/1770 train_time:13177ms step_avg:97.61ms
step:136/1770 train_time:13271ms step_avg:97.58ms
step:137/1770 train_time:13368ms step_avg:97.58ms
step:138/1770 train_time:13469ms step_avg:97.60ms
step:139/1770 train_time:13567ms step_avg:97.61ms
step:140/1770 train_time:13663ms step_avg:97.59ms
step:141/1770 train_time:13757ms step_avg:97.57ms
step:142/1770 train_time:13852ms step_avg:97.55ms
step:143/1770 train_time:13947ms step_avg:97.53ms
step:144/1770 train_time:14041ms step_avg:97.51ms
step:145/1770 train_time:14136ms step_avg:97.49ms
step:146/1770 train_time:14230ms step_avg:97.47ms
step:147/1770 train_time:14325ms step_avg:97.45ms
step:148/1770 train_time:14422ms step_avg:97.44ms
step:149/1770 train_time:14518ms step_avg:97.44ms
step:150/1770 train_time:14614ms step_avg:97.42ms
step:151/1770 train_time:14709ms step_avg:97.41ms
step:152/1770 train_time:14805ms step_avg:97.40ms
step:153/1770 train_time:14899ms step_avg:97.38ms
step:154/1770 train_time:14994ms step_avg:97.37ms
step:155/1770 train_time:15089ms step_avg:97.35ms
step:156/1770 train_time:15184ms step_avg:97.33ms
step:157/1770 train_time:15278ms step_avg:97.31ms
step:158/1770 train_time:15374ms step_avg:97.31ms
step:159/1770 train_time:15470ms step_avg:97.30ms
step:160/1770 train_time:15569ms step_avg:97.30ms
step:161/1770 train_time:15664ms step_avg:97.29ms
step:162/1770 train_time:15759ms step_avg:97.28ms
step:163/1770 train_time:15854ms step_avg:97.27ms
step:164/1770 train_time:15949ms step_avg:97.25ms
step:165/1770 train_time:16044ms step_avg:97.24ms
step:166/1770 train_time:16139ms step_avg:97.22ms
step:167/1770 train_time:16234ms step_avg:97.21ms
step:168/1770 train_time:16328ms step_avg:97.19ms
step:169/1770 train_time:16424ms step_avg:97.18ms
step:170/1770 train_time:16519ms step_avg:97.17ms
step:171/1770 train_time:16616ms step_avg:97.17ms
step:172/1770 train_time:16711ms step_avg:97.16ms
step:173/1770 train_time:16807ms step_avg:97.15ms
step:174/1770 train_time:16903ms step_avg:97.14ms
step:175/1770 train_time:16997ms step_avg:97.13ms
step:176/1770 train_time:17092ms step_avg:97.12ms
step:177/1770 train_time:17188ms step_avg:97.10ms
step:178/1770 train_time:17281ms step_avg:97.09ms
step:179/1770 train_time:17377ms step_avg:97.08ms
step:180/1770 train_time:17473ms step_avg:97.07ms
step:181/1770 train_time:17570ms step_avg:97.07ms
step:182/1770 train_time:17665ms step_avg:97.06ms
step:183/1770 train_time:17760ms step_avg:97.05ms
step:184/1770 train_time:17856ms step_avg:97.04ms
step:185/1770 train_time:17951ms step_avg:97.03ms
step:186/1770 train_time:18046ms step_avg:97.02ms
step:187/1770 train_time:18141ms step_avg:97.01ms
step:188/1770 train_time:18236ms step_avg:97.00ms
step:189/1770 train_time:18331ms step_avg:96.99ms
step:190/1770 train_time:18425ms step_avg:96.98ms
step:191/1770 train_time:18520ms step_avg:96.96ms
step:192/1770 train_time:18616ms step_avg:96.96ms
step:193/1770 train_time:18711ms step_avg:96.95ms
step:194/1770 train_time:18806ms step_avg:96.94ms
step:195/1770 train_time:18902ms step_avg:96.93ms
step:196/1770 train_time:18996ms step_avg:96.92ms
step:197/1770 train_time:19091ms step_avg:96.91ms
step:198/1770 train_time:19186ms step_avg:96.90ms
step:199/1770 train_time:19280ms step_avg:96.88ms
step:200/1770 train_time:19376ms step_avg:96.88ms
step:201/1770 train_time:19472ms step_avg:96.87ms
step:202/1770 train_time:19567ms step_avg:96.87ms
step:203/1770 train_time:19662ms step_avg:96.86ms
step:204/1770 train_time:19758ms step_avg:96.85ms
step:205/1770 train_time:19854ms step_avg:96.85ms
step:206/1770 train_time:19950ms step_avg:96.84ms
step:207/1770 train_time:20045ms step_avg:96.83ms
step:208/1770 train_time:20139ms step_avg:96.82ms
step:209/1770 train_time:20234ms step_avg:96.82ms
step:210/1770 train_time:20330ms step_avg:96.81ms
step:211/1770 train_time:20425ms step_avg:96.80ms
step:212/1770 train_time:20520ms step_avg:96.79ms
step:213/1770 train_time:20615ms step_avg:96.78ms
step:214/1770 train_time:20711ms step_avg:96.78ms
step:215/1770 train_time:20806ms step_avg:96.77ms
step:216/1770 train_time:20901ms step_avg:96.76ms
step:217/1770 train_time:20996ms step_avg:96.76ms
step:218/1770 train_time:21092ms step_avg:96.75ms
step:219/1770 train_time:21188ms step_avg:96.75ms
step:220/1770 train_time:21281ms step_avg:96.73ms
step:221/1770 train_time:21377ms step_avg:96.73ms
step:222/1770 train_time:21472ms step_avg:96.72ms
step:223/1770 train_time:21567ms step_avg:96.71ms
step:224/1770 train_time:21662ms step_avg:96.70ms
step:225/1770 train_time:21758ms step_avg:96.70ms
step:226/1770 train_time:21853ms step_avg:96.70ms
step:227/1770 train_time:21949ms step_avg:96.69ms
step:228/1770 train_time:22044ms step_avg:96.68ms
step:229/1770 train_time:22139ms step_avg:96.68ms
step:230/1770 train_time:22234ms step_avg:96.67ms
step:231/1770 train_time:22329ms step_avg:96.66ms
step:232/1770 train_time:22424ms step_avg:96.66ms
step:233/1770 train_time:22519ms step_avg:96.65ms
step:234/1770 train_time:22614ms step_avg:96.64ms
step:235/1770 train_time:22710ms step_avg:96.64ms
step:236/1770 train_time:22805ms step_avg:96.63ms
step:237/1770 train_time:22900ms step_avg:96.62ms
step:238/1770 train_time:22995ms step_avg:96.62ms
step:239/1770 train_time:23090ms step_avg:96.61ms
step:240/1770 train_time:23187ms step_avg:96.61ms
step:241/1770 train_time:23281ms step_avg:96.60ms
step:242/1770 train_time:23377ms step_avg:96.60ms
step:243/1770 train_time:23472ms step_avg:96.59ms
step:244/1770 train_time:23566ms step_avg:96.58ms
step:245/1770 train_time:23661ms step_avg:96.58ms
step:246/1770 train_time:23757ms step_avg:96.57ms
step:247/1770 train_time:23852ms step_avg:96.57ms
step:248/1770 train_time:23948ms step_avg:96.56ms
step:249/1770 train_time:24043ms step_avg:96.56ms
step:250/1770 train_time:24138ms step_avg:96.55ms
step:250/1770 val_loss:4.1075 train_time:24232ms step_avg:96.93ms
step:251/1770 train_time:24251ms step_avg:96.62ms
step:252/1770 train_time:24336ms step_avg:96.57ms
step:253/1770 train_time:24436ms step_avg:96.58ms
step:254/1770 train_time:24532ms step_avg:96.58ms
step:255/1770 train_time:24626ms step_avg:96.57ms
step:256/1770 train_time:24720ms step_avg:96.56ms
step:257/1770 train_time:24815ms step_avg:96.56ms
step:258/1770 train_time:24908ms step_avg:96.54ms
step:259/1770 train_time:25003ms step_avg:96.53ms
step:260/1770 train_time:25097ms step_avg:96.53ms
step:261/1770 train_time:25192ms step_avg:96.52ms
step:262/1770 train_time:25289ms step_avg:96.52ms
step:263/1770 train_time:25386ms step_avg:96.52ms
step:264/1770 train_time:25482ms step_avg:96.52ms
step:265/1770 train_time:25578ms step_avg:96.52ms
step:266/1770 train_time:25674ms step_avg:96.52ms
step:267/1770 train_time:25770ms step_avg:96.52ms
step:268/1770 train_time:25864ms step_avg:96.51ms
step:269/1770 train_time:25959ms step_avg:96.50ms
step:270/1770 train_time:26054ms step_avg:96.50ms
step:271/1770 train_time:26149ms step_avg:96.49ms
step:272/1770 train_time:26246ms step_avg:96.49ms
step:273/1770 train_time:26343ms step_avg:96.49ms
step:274/1770 train_time:26439ms step_avg:96.49ms
step:275/1770 train_time:26535ms step_avg:96.49ms
step:276/1770 train_time:26631ms step_avg:96.49ms
step:277/1770 train_time:26726ms step_avg:96.49ms
step:278/1770 train_time:26823ms step_avg:96.48ms
step:279/1770 train_time:26917ms step_avg:96.48ms
step:280/1770 train_time:27012ms step_avg:96.47ms
step:281/1770 train_time:27107ms step_avg:96.47ms
step:282/1770 train_time:27203ms step_avg:96.46ms
step:283/1770 train_time:27299ms step_avg:96.46ms
step:284/1770 train_time:27396ms step_avg:96.46ms
step:285/1770 train_time:27492ms step_avg:96.46ms
step:286/1770 train_time:27589ms step_avg:96.46ms
step:287/1770 train_time:27683ms step_avg:96.46ms
step:288/1770 train_time:27779ms step_avg:96.45ms
step:289/1770 train_time:27874ms step_avg:96.45ms
step:290/1770 train_time:27970ms step_avg:96.45ms
step:291/1770 train_time:28066ms step_avg:96.45ms
step:292/1770 train_time:28161ms step_avg:96.44ms
step:293/1770 train_time:28256ms step_avg:96.44ms
step:294/1770 train_time:28353ms step_avg:96.44ms
step:295/1770 train_time:28449ms step_avg:96.44ms
step:296/1770 train_time:28545ms step_avg:96.43ms
step:297/1770 train_time:28640ms step_avg:96.43ms
step:298/1770 train_time:28736ms step_avg:96.43ms
step:299/1770 train_time:28832ms step_avg:96.43ms
step:300/1770 train_time:28927ms step_avg:96.42ms
step:301/1770 train_time:29021ms step_avg:96.42ms
step:302/1770 train_time:29117ms step_avg:96.41ms
step:303/1770 train_time:29213ms step_avg:96.41ms
step:304/1770 train_time:29309ms step_avg:96.41ms
step:305/1770 train_time:29405ms step_avg:96.41ms
step:306/1770 train_time:29500ms step_avg:96.41ms
step:307/1770 train_time:29596ms step_avg:96.40ms
step:308/1770 train_time:29692ms step_avg:96.40ms
step:309/1770 train_time:29787ms step_avg:96.40ms
step:310/1770 train_time:29882ms step_avg:96.39ms
step:311/1770 train_time:29978ms step_avg:96.39ms
step:312/1770 train_time:30075ms step_avg:96.39ms
step:313/1770 train_time:30171ms step_avg:96.39ms
step:314/1770 train_time:30266ms step_avg:96.39ms
step:315/1770 train_time:30361ms step_avg:96.39ms
step:316/1770 train_time:30457ms step_avg:96.38ms
step:317/1770 train_time:30553ms step_avg:96.38ms
step:318/1770 train_time:30650ms step_avg:96.38ms
step:319/1770 train_time:30746ms step_avg:96.38ms
step:320/1770 train_time:30843ms step_avg:96.38ms
step:321/1770 train_time:30938ms step_avg:96.38ms
step:322/1770 train_time:31034ms step_avg:96.38ms
step:323/1770 train_time:31130ms step_avg:96.38ms
step:324/1770 train_time:31226ms step_avg:96.38ms
step:325/1770 train_time:31321ms step_avg:96.37ms
step:326/1770 train_time:31416ms step_avg:96.37ms
step:327/1770 train_time:31512ms step_avg:96.37ms
step:328/1770 train_time:31607ms step_avg:96.36ms
step:329/1770 train_time:31703ms step_avg:96.36ms
step:330/1770 train_time:31799ms step_avg:96.36ms
step:331/1770 train_time:31894ms step_avg:96.36ms
step:332/1770 train_time:31990ms step_avg:96.36ms
step:333/1770 train_time:32086ms step_avg:96.35ms
step:334/1770 train_time:32182ms step_avg:96.35ms
step:335/1770 train_time:32278ms step_avg:96.35ms
step:336/1770 train_time:32373ms step_avg:96.35ms
step:337/1770 train_time:32468ms step_avg:96.35ms
step:338/1770 train_time:32564ms step_avg:96.34ms
step:339/1770 train_time:32661ms step_avg:96.34ms
step:340/1770 train_time:32757ms step_avg:96.34ms
step:341/1770 train_time:32852ms step_avg:96.34ms
step:342/1770 train_time:32948ms step_avg:96.34ms
step:343/1770 train_time:33044ms step_avg:96.34ms
step:344/1770 train_time:33140ms step_avg:96.34ms
step:345/1770 train_time:33236ms step_avg:96.34ms
step:346/1770 train_time:33332ms step_avg:96.33ms
step:347/1770 train_time:33428ms step_avg:96.33ms
step:348/1770 train_time:33523ms step_avg:96.33ms
step:349/1770 train_time:33618ms step_avg:96.33ms
step:350/1770 train_time:33715ms step_avg:96.33ms
step:351/1770 train_time:33811ms step_avg:96.33ms
step:352/1770 train_time:33907ms step_avg:96.33ms
step:353/1770 train_time:34002ms step_avg:96.32ms
step:354/1770 train_time:34098ms step_avg:96.32ms
step:355/1770 train_time:34194ms step_avg:96.32ms
step:356/1770 train_time:34290ms step_avg:96.32ms
step:357/1770 train_time:34386ms step_avg:96.32ms
step:358/1770 train_time:34481ms step_avg:96.32ms
step:359/1770 train_time:34577ms step_avg:96.32ms
step:360/1770 train_time:34673ms step_avg:96.31ms
step:361/1770 train_time:34769ms step_avg:96.31ms
step:362/1770 train_time:34864ms step_avg:96.31ms
step:363/1770 train_time:34960ms step_avg:96.31ms
step:364/1770 train_time:35056ms step_avg:96.31ms
step:365/1770 train_time:35152ms step_avg:96.31ms
step:366/1770 train_time:35247ms step_avg:96.30ms
step:367/1770 train_time:35343ms step_avg:96.30ms
step:368/1770 train_time:35439ms step_avg:96.30ms
step:369/1770 train_time:35535ms step_avg:96.30ms
step:370/1770 train_time:35630ms step_avg:96.30ms
step:371/1770 train_time:35726ms step_avg:96.30ms
step:372/1770 train_time:35821ms step_avg:96.29ms
step:373/1770 train_time:35916ms step_avg:96.29ms
step:374/1770 train_time:36012ms step_avg:96.29ms
step:375/1770 train_time:36107ms step_avg:96.29ms
step:375/1770 val_loss:3.9071 train_time:36202ms step_avg:96.54ms
step:376/1770 train_time:36221ms step_avg:96.33ms
step:377/1770 train_time:36305ms step_avg:96.30ms
step:378/1770 train_time:36409ms step_avg:96.32ms
step:379/1770 train_time:36505ms step_avg:96.32ms
step:380/1770 train_time:36600ms step_avg:96.32ms
step:381/1770 train_time:36696ms step_avg:96.32ms
step:382/1770 train_time:36792ms step_avg:96.31ms
step:383/1770 train_time:36887ms step_avg:96.31ms
step:384/1770 train_time:36982ms step_avg:96.31ms
step:385/1770 train_time:37077ms step_avg:96.30ms
step:386/1770 train_time:37173ms step_avg:96.30ms
step:387/1770 train_time:37272ms step_avg:96.31ms
step:388/1770 train_time:37371ms step_avg:96.32ms
step:389/1770 train_time:37468ms step_avg:96.32ms
step:390/1770 train_time:37564ms step_avg:96.32ms
step:391/1770 train_time:37660ms step_avg:96.32ms
step:392/1770 train_time:37755ms step_avg:96.31ms
step:393/1770 train_time:37850ms step_avg:96.31ms
step:394/1770 train_time:37946ms step_avg:96.31ms
step:395/1770 train_time:38040ms step_avg:96.30ms
step:396/1770 train_time:38138ms step_avg:96.31ms
step:397/1770 train_time:38237ms step_avg:96.32ms
step:398/1770 train_time:38338ms step_avg:96.33ms
step:399/1770 train_time:38440ms step_avg:96.34ms
step:400/1770 train_time:38539ms step_avg:96.35ms
step:401/1770 train_time:38639ms step_avg:96.36ms
step:402/1770 train_time:38736ms step_avg:96.36ms
step:403/1770 train_time:38834ms step_avg:96.36ms
step:404/1770 train_time:38932ms step_avg:96.37ms
step:405/1770 train_time:39030ms step_avg:96.37ms
step:406/1770 train_time:39128ms step_avg:96.37ms
step:407/1770 train_time:39225ms step_avg:96.38ms
step:408/1770 train_time:39324ms step_avg:96.38ms
step:409/1770 train_time:39422ms step_avg:96.39ms
step:410/1770 train_time:39520ms step_avg:96.39ms
step:411/1770 train_time:39619ms step_avg:96.40ms
step:412/1770 train_time:39719ms step_avg:96.41ms
step:413/1770 train_time:39818ms step_avg:96.41ms
step:414/1770 train_time:39916ms step_avg:96.42ms
step:415/1770 train_time:40015ms step_avg:96.42ms
step:416/1770 train_time:40113ms step_avg:96.43ms
step:417/1770 train_time:40211ms step_avg:96.43ms
step:418/1770 train_time:40310ms step_avg:96.43ms
step:419/1770 train_time:40409ms step_avg:96.44ms
step:420/1770 train_time:40508ms step_avg:96.45ms
step:421/1770 train_time:40607ms step_avg:96.45ms
step:422/1770 train_time:40706ms step_avg:96.46ms
step:423/1770 train_time:40804ms step_avg:96.46ms
step:424/1770 train_time:40903ms step_avg:96.47ms
step:425/1770 train_time:41001ms step_avg:96.47ms
step:426/1770 train_time:41100ms step_avg:96.48ms
step:427/1770 train_time:41199ms step_avg:96.48ms
step:428/1770 train_time:41298ms step_avg:96.49ms
step:429/1770 train_time:41397ms step_avg:96.50ms
step:430/1770 train_time:41497ms step_avg:96.50ms
step:431/1770 train_time:41597ms step_avg:96.51ms
step:432/1770 train_time:41696ms step_avg:96.52ms
step:433/1770 train_time:41795ms step_avg:96.52ms
step:434/1770 train_time:41893ms step_avg:96.53ms
step:435/1770 train_time:41991ms step_avg:96.53ms
step:436/1770 train_time:42089ms step_avg:96.53ms
step:437/1770 train_time:42187ms step_avg:96.54ms
step:438/1770 train_time:42285ms step_avg:96.54ms
step:439/1770 train_time:42383ms step_avg:96.54ms
step:440/1770 train_time:42482ms step_avg:96.55ms
step:441/1770 train_time:42580ms step_avg:96.55ms
step:442/1770 train_time:42680ms step_avg:96.56ms
step:443/1770 train_time:42779ms step_avg:96.57ms
step:444/1770 train_time:42878ms step_avg:96.57ms
step:445/1770 train_time:42977ms step_avg:96.58ms
step:446/1770 train_time:43075ms step_avg:96.58ms
step:447/1770 train_time:43174ms step_avg:96.59ms
step:448/1770 train_time:43273ms step_avg:96.59ms
step:449/1770 train_time:43372ms step_avg:96.60ms
step:450/1770 train_time:43471ms step_avg:96.60ms
step:451/1770 train_time:43569ms step_avg:96.61ms
step:452/1770 train_time:43666ms step_avg:96.61ms
step:453/1770 train_time:43763ms step_avg:96.61ms
step:454/1770 train_time:43861ms step_avg:96.61ms
step:455/1770 train_time:43960ms step_avg:96.61ms
step:456/1770 train_time:44058ms step_avg:96.62ms
step:457/1770 train_time:44157ms step_avg:96.62ms
step:458/1770 train_time:44256ms step_avg:96.63ms
step:459/1770 train_time:44355ms step_avg:96.63ms
step:460/1770 train_time:44454ms step_avg:96.64ms
step:461/1770 train_time:44553ms step_avg:96.64ms
step:462/1770 train_time:44651ms step_avg:96.65ms
step:463/1770 train_time:44750ms step_avg:96.65ms
step:464/1770 train_time:44848ms step_avg:96.66ms
step:465/1770 train_time:44946ms step_avg:96.66ms
step:466/1770 train_time:45045ms step_avg:96.66ms
step:467/1770 train_time:45141ms step_avg:96.66ms
step:468/1770 train_time:45240ms step_avg:96.67ms
step:469/1770 train_time:45340ms step_avg:96.67ms
step:470/1770 train_time:45438ms step_avg:96.68ms
step:471/1770 train_time:45536ms step_avg:96.68ms
step:472/1770 train_time:45635ms step_avg:96.68ms
step:473/1770 train_time:45734ms step_avg:96.69ms
step:474/1770 train_time:45833ms step_avg:96.69ms
step:475/1770 train_time:45931ms step_avg:96.70ms
step:476/1770 train_time:46029ms step_avg:96.70ms
step:477/1770 train_time:46127ms step_avg:96.70ms
step:478/1770 train_time:46225ms step_avg:96.71ms
step:479/1770 train_time:46324ms step_avg:96.71ms
step:480/1770 train_time:46421ms step_avg:96.71ms
step:481/1770 train_time:46519ms step_avg:96.71ms
step:482/1770 train_time:46618ms step_avg:96.72ms
step:483/1770 train_time:46717ms step_avg:96.72ms
step:484/1770 train_time:46816ms step_avg:96.73ms
step:485/1770 train_time:46914ms step_avg:96.73ms
step:486/1770 train_time:47013ms step_avg:96.73ms
step:487/1770 train_time:47111ms step_avg:96.74ms
step:488/1770 train_time:47209ms step_avg:96.74ms
step:489/1770 train_time:47307ms step_avg:96.74ms
step:490/1770 train_time:47405ms step_avg:96.74ms
step:491/1770 train_time:47502ms step_avg:96.74ms
step:492/1770 train_time:47599ms step_avg:96.75ms
step:493/1770 train_time:47697ms step_avg:96.75ms
step:494/1770 train_time:47795ms step_avg:96.75ms
step:495/1770 train_time:47893ms step_avg:96.75ms
step:496/1770 train_time:47991ms step_avg:96.76ms
step:497/1770 train_time:48090ms step_avg:96.76ms
step:498/1770 train_time:48188ms step_avg:96.76ms
step:499/1770 train_time:48286ms step_avg:96.77ms
step:500/1770 train_time:48383ms step_avg:96.77ms
step:500/1770 val_loss:3.7560 train_time:48481ms step_avg:96.96ms
step:501/1770 train_time:48499ms step_avg:96.80ms
step:502/1770 train_time:48587ms step_avg:96.79ms
step:503/1770 train_time:48690ms step_avg:96.80ms
step:504/1770 train_time:48789ms step_avg:96.80ms
step:505/1770 train_time:48886ms step_avg:96.80ms
step:506/1770 train_time:48983ms step_avg:96.80ms
step:507/1770 train_time:49080ms step_avg:96.81ms
step:508/1770 train_time:49177ms step_avg:96.81ms
step:509/1770 train_time:49275ms step_avg:96.81ms
step:510/1770 train_time:49372ms step_avg:96.81ms
step:511/1770 train_time:49471ms step_avg:96.81ms
step:512/1770 train_time:49571ms step_avg:96.82ms
step:513/1770 train_time:49673ms step_avg:96.83ms
step:514/1770 train_time:49772ms step_avg:96.83ms
step:515/1770 train_time:49870ms step_avg:96.84ms
step:516/1770 train_time:49969ms step_avg:96.84ms
step:517/1770 train_time:50067ms step_avg:96.84ms
step:518/1770 train_time:50163ms step_avg:96.84ms
step:519/1770 train_time:50261ms step_avg:96.84ms
step:520/1770 train_time:50358ms step_avg:96.84ms
step:521/1770 train_time:50456ms step_avg:96.84ms
step:522/1770 train_time:50555ms step_avg:96.85ms
step:523/1770 train_time:50655ms step_avg:96.85ms
step:524/1770 train_time:50754ms step_avg:96.86ms
step:525/1770 train_time:50853ms step_avg:96.86ms
step:526/1770 train_time:50951ms step_avg:96.86ms
step:527/1770 train_time:51051ms step_avg:96.87ms
step:528/1770 train_time:51148ms step_avg:96.87ms
step:529/1770 train_time:51247ms step_avg:96.88ms
step:530/1770 train_time:51345ms step_avg:96.88ms
step:531/1770 train_time:51444ms step_avg:96.88ms
step:532/1770 train_time:51543ms step_avg:96.89ms
step:533/1770 train_time:51641ms step_avg:96.89ms
step:534/1770 train_time:51740ms step_avg:96.89ms
step:535/1770 train_time:51839ms step_avg:96.90ms
step:536/1770 train_time:51940ms step_avg:96.90ms
step:537/1770 train_time:52040ms step_avg:96.91ms
step:538/1770 train_time:52139ms step_avg:96.91ms
step:539/1770 train_time:52238ms step_avg:96.92ms
step:540/1770 train_time:52337ms step_avg:96.92ms
step:541/1770 train_time:52435ms step_avg:96.92ms
step:542/1770 train_time:52534ms step_avg:96.93ms
step:543/1770 train_time:52632ms step_avg:96.93ms
step:544/1770 train_time:52731ms step_avg:96.93ms
step:545/1770 train_time:52830ms step_avg:96.94ms
step:546/1770 train_time:52929ms step_avg:96.94ms
step:547/1770 train_time:53028ms step_avg:96.94ms
step:548/1770 train_time:53127ms step_avg:96.95ms
step:549/1770 train_time:53226ms step_avg:96.95ms
step:550/1770 train_time:53324ms step_avg:96.95ms
step:551/1770 train_time:53421ms step_avg:96.95ms
step:552/1770 train_time:53519ms step_avg:96.96ms
step:553/1770 train_time:53618ms step_avg:96.96ms
step:554/1770 train_time:53716ms step_avg:96.96ms
step:555/1770 train_time:53816ms step_avg:96.97ms
step:556/1770 train_time:53915ms step_avg:96.97ms
step:557/1770 train_time:54015ms step_avg:96.98ms
step:558/1770 train_time:54115ms step_avg:96.98ms
step:559/1770 train_time:54214ms step_avg:96.98ms
step:560/1770 train_time:54312ms step_avg:96.99ms
step:561/1770 train_time:54410ms step_avg:96.99ms
step:562/1770 train_time:54508ms step_avg:96.99ms
step:563/1770 train_time:54606ms step_avg:96.99ms
step:564/1770 train_time:54705ms step_avg:96.99ms
step:565/1770 train_time:54802ms step_avg:97.00ms
step:566/1770 train_time:54901ms step_avg:97.00ms
step:567/1770 train_time:55000ms step_avg:97.00ms
step:568/1770 train_time:55099ms step_avg:97.01ms
step:569/1770 train_time:55198ms step_avg:97.01ms
step:570/1770 train_time:55298ms step_avg:97.01ms
step:571/1770 train_time:55397ms step_avg:97.02ms
step:572/1770 train_time:55497ms step_avg:97.02ms
step:573/1770 train_time:55595ms step_avg:97.02ms
step:574/1770 train_time:55695ms step_avg:97.03ms
step:575/1770 train_time:55792ms step_avg:97.03ms
step:576/1770 train_time:55891ms step_avg:97.03ms
step:577/1770 train_time:55989ms step_avg:97.04ms
step:578/1770 train_time:56088ms step_avg:97.04ms
step:579/1770 train_time:56186ms step_avg:97.04ms
step:580/1770 train_time:56285ms step_avg:97.04ms
step:581/1770 train_time:56386ms step_avg:97.05ms
step:582/1770 train_time:56484ms step_avg:97.05ms
step:583/1770 train_time:56582ms step_avg:97.05ms
step:584/1770 train_time:56680ms step_avg:97.05ms
step:585/1770 train_time:56779ms step_avg:97.06ms
step:586/1770 train_time:56877ms step_avg:97.06ms
step:587/1770 train_time:56976ms step_avg:97.06ms
step:588/1770 train_time:57076ms step_avg:97.07ms
step:589/1770 train_time:57175ms step_avg:97.07ms
step:590/1770 train_time:57274ms step_avg:97.08ms
step:591/1770 train_time:57373ms step_avg:97.08ms
step:592/1770 train_time:57472ms step_avg:97.08ms
step:593/1770 train_time:57571ms step_avg:97.09ms
step:594/1770 train_time:57669ms step_avg:97.09ms
step:595/1770 train_time:57768ms step_avg:97.09ms
step:596/1770 train_time:57866ms step_avg:97.09ms
step:597/1770 train_time:57964ms step_avg:97.09ms
step:598/1770 train_time:58063ms step_avg:97.09ms
step:599/1770 train_time:58160ms step_avg:97.10ms
step:600/1770 train_time:58259ms step_avg:97.10ms
step:601/1770 train_time:58358ms step_avg:97.10ms
step:602/1770 train_time:58456ms step_avg:97.10ms
step:603/1770 train_time:58554ms step_avg:97.11ms
step:604/1770 train_time:58652ms step_avg:97.11ms
step:605/1770 train_time:58751ms step_avg:97.11ms
step:606/1770 train_time:58850ms step_avg:97.11ms
step:607/1770 train_time:58950ms step_avg:97.12ms
step:608/1770 train_time:59049ms step_avg:97.12ms
step:609/1770 train_time:59147ms step_avg:97.12ms
step:610/1770 train_time:59245ms step_avg:97.12ms
step:611/1770 train_time:59343ms step_avg:97.12ms
step:612/1770 train_time:59442ms step_avg:97.13ms
step:613/1770 train_time:59540ms step_avg:97.13ms
step:614/1770 train_time:59639ms step_avg:97.13ms
step:615/1770 train_time:59738ms step_avg:97.14ms
step:616/1770 train_time:59838ms step_avg:97.14ms
step:617/1770 train_time:59939ms step_avg:97.15ms
step:618/1770 train_time:60038ms step_avg:97.15ms
step:619/1770 train_time:60137ms step_avg:97.15ms
step:620/1770 train_time:60235ms step_avg:97.15ms
step:621/1770 train_time:60334ms step_avg:97.16ms
step:622/1770 train_time:60432ms step_avg:97.16ms
step:623/1770 train_time:60530ms step_avg:97.16ms
step:624/1770 train_time:60627ms step_avg:97.16ms
step:625/1770 train_time:60725ms step_avg:97.16ms
step:625/1770 val_loss:3.6708 train_time:60823ms step_avg:97.32ms
step:626/1770 train_time:60842ms step_avg:97.19ms
step:627/1770 train_time:60934ms step_avg:97.18ms
step:628/1770 train_time:61034ms step_avg:97.19ms
step:629/1770 train_time:61132ms step_avg:97.19ms
step:630/1770 train_time:61230ms step_avg:97.19ms
step:631/1770 train_time:61328ms step_avg:97.19ms
step:632/1770 train_time:61425ms step_avg:97.19ms
step:633/1770 train_time:61522ms step_avg:97.19ms
step:634/1770 train_time:61619ms step_avg:97.19ms
step:635/1770 train_time:61717ms step_avg:97.19ms
step:636/1770 train_time:61818ms step_avg:97.20ms
step:637/1770 train_time:61919ms step_avg:97.20ms
step:638/1770 train_time:62019ms step_avg:97.21ms
step:639/1770 train_time:62119ms step_avg:97.21ms
step:640/1770 train_time:62219ms step_avg:97.22ms
step:641/1770 train_time:62318ms step_avg:97.22ms
step:642/1770 train_time:62417ms step_avg:97.22ms
step:643/1770 train_time:62516ms step_avg:97.23ms
step:644/1770 train_time:62614ms step_avg:97.23ms
step:645/1770 train_time:62712ms step_avg:97.23ms
step:646/1770 train_time:62812ms step_avg:97.23ms
step:647/1770 train_time:62912ms step_avg:97.24ms
step:648/1770 train_time:63013ms step_avg:97.24ms
step:649/1770 train_time:63111ms step_avg:97.24ms
step:650/1770 train_time:63209ms step_avg:97.25ms
step:651/1770 train_time:63307ms step_avg:97.25ms
step:652/1770 train_time:63406ms step_avg:97.25ms
step:653/1770 train_time:63504ms step_avg:97.25ms
step:654/1770 train_time:63602ms step_avg:97.25ms
step:655/1770 train_time:63700ms step_avg:97.25ms
step:656/1770 train_time:63799ms step_avg:97.25ms
step:657/1770 train_time:63897ms step_avg:97.26ms
step:658/1770 train_time:63998ms step_avg:97.26ms
step:659/1770 train_time:64099ms step_avg:97.27ms
step:660/1770 train_time:64200ms step_avg:97.27ms
step:661/1770 train_time:64302ms step_avg:97.28ms
step:662/1770 train_time:64403ms step_avg:97.29ms
step:663/1770 train_time:64504ms step_avg:97.29ms
step:664/1770 train_time:64603ms step_avg:97.29ms
step:665/1770 train_time:64702ms step_avg:97.30ms
step:666/1770 train_time:64802ms step_avg:97.30ms
step:667/1770 train_time:64902ms step_avg:97.30ms
step:668/1770 train_time:65003ms step_avg:97.31ms
step:669/1770 train_time:65103ms step_avg:97.31ms
step:670/1770 train_time:65203ms step_avg:97.32ms
step:671/1770 train_time:65302ms step_avg:97.32ms
step:672/1770 train_time:65402ms step_avg:97.33ms
step:673/1770 train_time:65502ms step_avg:97.33ms
step:674/1770 train_time:65603ms step_avg:97.33ms
step:675/1770 train_time:65703ms step_avg:97.34ms
step:676/1770 train_time:65803ms step_avg:97.34ms
step:677/1770 train_time:65903ms step_avg:97.35ms
step:678/1770 train_time:66003ms step_avg:97.35ms
step:679/1770 train_time:66104ms step_avg:97.35ms
step:680/1770 train_time:66205ms step_avg:97.36ms
step:681/1770 train_time:66305ms step_avg:97.36ms
step:682/1770 train_time:66406ms step_avg:97.37ms
step:683/1770 train_time:66507ms step_avg:97.37ms
step:684/1770 train_time:66607ms step_avg:97.38ms
step:685/1770 train_time:66708ms step_avg:97.38ms
step:686/1770 train_time:66809ms step_avg:97.39ms
step:687/1770 train_time:66909ms step_avg:97.39ms
step:688/1770 train_time:67009ms step_avg:97.40ms
step:689/1770 train_time:67110ms step_avg:97.40ms
step:690/1770 train_time:67210ms step_avg:97.41ms
step:691/1770 train_time:67311ms step_avg:97.41ms
step:692/1770 train_time:67412ms step_avg:97.42ms
step:693/1770 train_time:67512ms step_avg:97.42ms
step:694/1770 train_time:67613ms step_avg:97.42ms
step:695/1770 train_time:67712ms step_avg:97.43ms
step:696/1770 train_time:67813ms step_avg:97.43ms
step:697/1770 train_time:67914ms step_avg:97.44ms
step:698/1770 train_time:68015ms step_avg:97.44ms
step:699/1770 train_time:68116ms step_avg:97.45ms
step:700/1770 train_time:68217ms step_avg:97.45ms
step:701/1770 train_time:68318ms step_avg:97.46ms
step:702/1770 train_time:68420ms step_avg:97.46ms
step:703/1770 train_time:68521ms step_avg:97.47ms
step:704/1770 train_time:68622ms step_avg:97.47ms
step:705/1770 train_time:68722ms step_avg:97.48ms
step:706/1770 train_time:68822ms step_avg:97.48ms
step:707/1770 train_time:68923ms step_avg:97.49ms
step:708/1770 train_time:69023ms step_avg:97.49ms
step:709/1770 train_time:69124ms step_avg:97.49ms
step:710/1770 train_time:69224ms step_avg:97.50ms
step:711/1770 train_time:69324ms step_avg:97.50ms
step:712/1770 train_time:69424ms step_avg:97.51ms
step:713/1770 train_time:69524ms step_avg:97.51ms
step:714/1770 train_time:69624ms step_avg:97.51ms
step:715/1770 train_time:69724ms step_avg:97.52ms
step:716/1770 train_time:69824ms step_avg:97.52ms
step:717/1770 train_time:69924ms step_avg:97.52ms
step:718/1770 train_time:70025ms step_avg:97.53ms
step:719/1770 train_time:70126ms step_avg:97.53ms
step:720/1770 train_time:70226ms step_avg:97.54ms
step:721/1770 train_time:70327ms step_avg:97.54ms
step:722/1770 train_time:70427ms step_avg:97.54ms
step:723/1770 train_time:70528ms step_avg:97.55ms
step:724/1770 train_time:70629ms step_avg:97.55ms
step:725/1770 train_time:70729ms step_avg:97.56ms
step:726/1770 train_time:70829ms step_avg:97.56ms
step:727/1770 train_time:70929ms step_avg:97.56ms
step:728/1770 train_time:71029ms step_avg:97.57ms
step:729/1770 train_time:71129ms step_avg:97.57ms
step:730/1770 train_time:71229ms step_avg:97.57ms
step:731/1770 train_time:71329ms step_avg:97.58ms
step:732/1770 train_time:71429ms step_avg:97.58ms
step:733/1770 train_time:71530ms step_avg:97.58ms
step:734/1770 train_time:71631ms step_avg:97.59ms
step:735/1770 train_time:71731ms step_avg:97.59ms
step:736/1770 train_time:71832ms step_avg:97.60ms
step:737/1770 train_time:71932ms step_avg:97.60ms
step:738/1770 train_time:72033ms step_avg:97.61ms
step:739/1770 train_time:72133ms step_avg:97.61ms
step:740/1770 train_time:72234ms step_avg:97.61ms
step:741/1770 train_time:72335ms step_avg:97.62ms
step:742/1770 train_time:72435ms step_avg:97.62ms
step:743/1770 train_time:72536ms step_avg:97.63ms
step:744/1770 train_time:72637ms step_avg:97.63ms
step:745/1770 train_time:72738ms step_avg:97.64ms
step:746/1770 train_time:72839ms step_avg:97.64ms
step:747/1770 train_time:72940ms step_avg:97.64ms
step:748/1770 train_time:73040ms step_avg:97.65ms
step:749/1770 train_time:73141ms step_avg:97.65ms
step:750/1770 train_time:73242ms step_avg:97.66ms
step:750/1770 val_loss:3.6078 train_time:73342ms step_avg:97.79ms
step:751/1770 train_time:73360ms step_avg:97.68ms
step:752/1770 train_time:73453ms step_avg:97.68ms
step:753/1770 train_time:73554ms step_avg:97.68ms
step:754/1770 train_time:73653ms step_avg:97.68ms
step:755/1770 train_time:73753ms step_avg:97.69ms
step:756/1770 train_time:73852ms step_avg:97.69ms
step:757/1770 train_time:73951ms step_avg:97.69ms
step:758/1770 train_time:74051ms step_avg:97.69ms
step:759/1770 train_time:74151ms step_avg:97.70ms
step:760/1770 train_time:74251ms step_avg:97.70ms
step:761/1770 train_time:74354ms step_avg:97.71ms
step:762/1770 train_time:74458ms step_avg:97.71ms
step:763/1770 train_time:74559ms step_avg:97.72ms
step:764/1770 train_time:74658ms step_avg:97.72ms
step:765/1770 train_time:74758ms step_avg:97.72ms
step:766/1770 train_time:74858ms step_avg:97.73ms
step:767/1770 train_time:74959ms step_avg:97.73ms
step:768/1770 train_time:75059ms step_avg:97.73ms
step:769/1770 train_time:75160ms step_avg:97.74ms
step:770/1770 train_time:75261ms step_avg:97.74ms
step:771/1770 train_time:75361ms step_avg:97.74ms
step:772/1770 train_time:75462ms step_avg:97.75ms
step:773/1770 train_time:75562ms step_avg:97.75ms
step:774/1770 train_time:75662ms step_avg:97.75ms
step:775/1770 train_time:75763ms step_avg:97.76ms
step:776/1770 train_time:75862ms step_avg:97.76ms
step:777/1770 train_time:75962ms step_avg:97.76ms
step:778/1770 train_time:76062ms step_avg:97.77ms
step:779/1770 train_time:76162ms step_avg:97.77ms
step:780/1770 train_time:76262ms step_avg:97.77ms
step:781/1770 train_time:76363ms step_avg:97.78ms
step:782/1770 train_time:76464ms step_avg:97.78ms
step:783/1770 train_time:76565ms step_avg:97.78ms
step:784/1770 train_time:76665ms step_avg:97.79ms
step:785/1770 train_time:76765ms step_avg:97.79ms
step:786/1770 train_time:76865ms step_avg:97.79ms
step:787/1770 train_time:76965ms step_avg:97.80ms
step:788/1770 train_time:77066ms step_avg:97.80ms
step:789/1770 train_time:77167ms step_avg:97.80ms
step:790/1770 train_time:77268ms step_avg:97.81ms
step:791/1770 train_time:77369ms step_avg:97.81ms
step:792/1770 train_time:77470ms step_avg:97.82ms
step:793/1770 train_time:77570ms step_avg:97.82ms
step:794/1770 train_time:77672ms step_avg:97.82ms
step:795/1770 train_time:77773ms step_avg:97.83ms
step:796/1770 train_time:77873ms step_avg:97.83ms
step:797/1770 train_time:77975ms step_avg:97.84ms
step:798/1770 train_time:78075ms step_avg:97.84ms
step:799/1770 train_time:78176ms step_avg:97.84ms
step:800/1770 train_time:78276ms step_avg:97.85ms
step:801/1770 train_time:78376ms step_avg:97.85ms
step:802/1770 train_time:78477ms step_avg:97.85ms
step:803/1770 train_time:78577ms step_avg:97.85ms
step:804/1770 train_time:78677ms step_avg:97.86ms
step:805/1770 train_time:78778ms step_avg:97.86ms
step:806/1770 train_time:78878ms step_avg:97.86ms
step:807/1770 train_time:78979ms step_avg:97.87ms
step:808/1770 train_time:79080ms step_avg:97.87ms
step:809/1770 train_time:79180ms step_avg:97.87ms
step:810/1770 train_time:79281ms step_avg:97.88ms
step:811/1770 train_time:79382ms step_avg:97.88ms
step:812/1770 train_time:79482ms step_avg:97.88ms
step:813/1770 train_time:79583ms step_avg:97.89ms
step:814/1770 train_time:79684ms step_avg:97.89ms
step:815/1770 train_time:79785ms step_avg:97.90ms
step:816/1770 train_time:79885ms step_avg:97.90ms
step:817/1770 train_time:79987ms step_avg:97.90ms
step:818/1770 train_time:80088ms step_avg:97.91ms
step:819/1770 train_time:80189ms step_avg:97.91ms
step:820/1770 train_time:80291ms step_avg:97.92ms
step:821/1770 train_time:80393ms step_avg:97.92ms
step:822/1770 train_time:80494ms step_avg:97.92ms
step:823/1770 train_time:80594ms step_avg:97.93ms
step:824/1770 train_time:80695ms step_avg:97.93ms
step:825/1770 train_time:80796ms step_avg:97.93ms
step:826/1770 train_time:80896ms step_avg:97.94ms
step:827/1770 train_time:80997ms step_avg:97.94ms
step:828/1770 train_time:81097ms step_avg:97.94ms
step:829/1770 train_time:81197ms step_avg:97.95ms
step:830/1770 train_time:81299ms step_avg:97.95ms
step:831/1770 train_time:81400ms step_avg:97.95ms
step:832/1770 train_time:81501ms step_avg:97.96ms
step:833/1770 train_time:81601ms step_avg:97.96ms
step:834/1770 train_time:81702ms step_avg:97.96ms
step:835/1770 train_time:81802ms step_avg:97.97ms
step:836/1770 train_time:81903ms step_avg:97.97ms
step:837/1770 train_time:82003ms step_avg:97.97ms
step:838/1770 train_time:82104ms step_avg:97.98ms
step:839/1770 train_time:82205ms step_avg:97.98ms
step:840/1770 train_time:82306ms step_avg:97.98ms
step:841/1770 train_time:82406ms step_avg:97.99ms
step:842/1770 train_time:82507ms step_avg:97.99ms
step:843/1770 train_time:82608ms step_avg:97.99ms
step:844/1770 train_time:82709ms step_avg:98.00ms
step:845/1770 train_time:82810ms step_avg:98.00ms
step:846/1770 train_time:82911ms step_avg:98.00ms
step:847/1770 train_time:83013ms step_avg:98.01ms
step:848/1770 train_time:83113ms step_avg:98.01ms
step:849/1770 train_time:83214ms step_avg:98.01ms
step:850/1770 train_time:83315ms step_avg:98.02ms
step:851/1770 train_time:83416ms step_avg:98.02ms
step:852/1770 train_time:83516ms step_avg:98.02ms
step:853/1770 train_time:83616ms step_avg:98.03ms
step:854/1770 train_time:83716ms step_avg:98.03ms
step:855/1770 train_time:83817ms step_avg:98.03ms
step:856/1770 train_time:83917ms step_avg:98.03ms
step:857/1770 train_time:84018ms step_avg:98.04ms
step:858/1770 train_time:84119ms step_avg:98.04ms
step:859/1770 train_time:84220ms step_avg:98.04ms
step:860/1770 train_time:84321ms step_avg:98.05ms
step:861/1770 train_time:84422ms step_avg:98.05ms
step:862/1770 train_time:84522ms step_avg:98.05ms
step:863/1770 train_time:84623ms step_avg:98.06ms
step:864/1770 train_time:84723ms step_avg:98.06ms
step:865/1770 train_time:84824ms step_avg:98.06ms
step:866/1770 train_time:84925ms step_avg:98.07ms
step:867/1770 train_time:85026ms step_avg:98.07ms
step:868/1770 train_time:85127ms step_avg:98.07ms
step:869/1770 train_time:85227ms step_avg:98.07ms
step:870/1770 train_time:85329ms step_avg:98.08ms
step:871/1770 train_time:85430ms step_avg:98.08ms
step:872/1770 train_time:85531ms step_avg:98.09ms
step:873/1770 train_time:85633ms step_avg:98.09ms
step:874/1770 train_time:85733ms step_avg:98.09ms
step:875/1770 train_time:85833ms step_avg:98.10ms
step:875/1770 val_loss:3.5565 train_time:85934ms step_avg:98.21ms
step:876/1770 train_time:85952ms step_avg:98.12ms
step:877/1770 train_time:86045ms step_avg:98.11ms
step:878/1770 train_time:86148ms step_avg:98.12ms
step:879/1770 train_time:86249ms step_avg:98.12ms
step:880/1770 train_time:86350ms step_avg:98.13ms
step:881/1770 train_time:86450ms step_avg:98.13ms
step:882/1770 train_time:86550ms step_avg:98.13ms
step:883/1770 train_time:86650ms step_avg:98.13ms
step:884/1770 train_time:86749ms step_avg:98.13ms
step:885/1770 train_time:86849ms step_avg:98.13ms
step:886/1770 train_time:86951ms step_avg:98.14ms
step:887/1770 train_time:87054ms step_avg:98.14ms
step:888/1770 train_time:87156ms step_avg:98.15ms
step:889/1770 train_time:87256ms step_avg:98.15ms
step:890/1770 train_time:87357ms step_avg:98.15ms
step:891/1770 train_time:87458ms step_avg:98.16ms
step:892/1770 train_time:87558ms step_avg:98.16ms
step:893/1770 train_time:87658ms step_avg:98.16ms
step:894/1770 train_time:87759ms step_avg:98.16ms
step:895/1770 train_time:87860ms step_avg:98.17ms
step:896/1770 train_time:87962ms step_avg:98.17ms
step:897/1770 train_time:88062ms step_avg:98.17ms
step:898/1770 train_time:88164ms step_avg:98.18ms
step:899/1770 train_time:88265ms step_avg:98.18ms
step:900/1770 train_time:88366ms step_avg:98.18ms
step:901/1770 train_time:88468ms step_avg:98.19ms
step:902/1770 train_time:88570ms step_avg:98.19ms
step:903/1770 train_time:88671ms step_avg:98.20ms
step:904/1770 train_time:88772ms step_avg:98.20ms
step:905/1770 train_time:88872ms step_avg:98.20ms
step:906/1770 train_time:88973ms step_avg:98.20ms
step:907/1770 train_time:89073ms step_avg:98.21ms
step:908/1770 train_time:89173ms step_avg:98.21ms
step:909/1770 train_time:89274ms step_avg:98.21ms
step:910/1770 train_time:89374ms step_avg:98.21ms
step:911/1770 train_time:89474ms step_avg:98.22ms
step:912/1770 train_time:89575ms step_avg:98.22ms
step:913/1770 train_time:89675ms step_avg:98.22ms
step:914/1770 train_time:89776ms step_avg:98.22ms
step:915/1770 train_time:89877ms step_avg:98.23ms
step:916/1770 train_time:89977ms step_avg:98.23ms
step:917/1770 train_time:90078ms step_avg:98.23ms
step:918/1770 train_time:90178ms step_avg:98.23ms
step:919/1770 train_time:90280ms step_avg:98.24ms
step:920/1770 train_time:90382ms step_avg:98.24ms
step:921/1770 train_time:90484ms step_avg:98.25ms
step:922/1770 train_time:90587ms step_avg:98.25ms
step:923/1770 train_time:90690ms step_avg:98.26ms
step:924/1770 train_time:90793ms step_avg:98.26ms
step:925/1770 train_time:90895ms step_avg:98.26ms
step:926/1770 train_time:90996ms step_avg:98.27ms
step:927/1770 train_time:91098ms step_avg:98.27ms
step:928/1770 train_time:91200ms step_avg:98.28ms
step:929/1770 train_time:91301ms step_avg:98.28ms
step:930/1770 train_time:91404ms step_avg:98.28ms
step:931/1770 train_time:91505ms step_avg:98.29ms
step:932/1770 train_time:91607ms step_avg:98.29ms
step:933/1770 train_time:91710ms step_avg:98.30ms
step:934/1770 train_time:91812ms step_avg:98.30ms
step:935/1770 train_time:91915ms step_avg:98.30ms
step:936/1770 train_time:92017ms step_avg:98.31ms
step:937/1770 train_time:92118ms step_avg:98.31ms
step:938/1770 train_time:92220ms step_avg:98.32ms
step:939/1770 train_time:92321ms step_avg:98.32ms
step:940/1770 train_time:92424ms step_avg:98.32ms
step:941/1770 train_time:92525ms step_avg:98.33ms
step:942/1770 train_time:92628ms step_avg:98.33ms
step:943/1770 train_time:92731ms step_avg:98.34ms
step:944/1770 train_time:92834ms step_avg:98.34ms
step:945/1770 train_time:92937ms step_avg:98.35ms
step:946/1770 train_time:93039ms step_avg:98.35ms
step:947/1770 train_time:93141ms step_avg:98.35ms
step:948/1770 train_time:93242ms step_avg:98.36ms
step:949/1770 train_time:93344ms step_avg:98.36ms
step:950/1770 train_time:93447ms step_avg:98.36ms
step:951/1770 train_time:93548ms step_avg:98.37ms
step:952/1770 train_time:93651ms step_avg:98.37ms
step:953/1770 train_time:93753ms step_avg:98.38ms
step:954/1770 train_time:93855ms step_avg:98.38ms
step:955/1770 train_time:93957ms step_avg:98.38ms
step:956/1770 train_time:94058ms step_avg:98.39ms
step:957/1770 train_time:94160ms step_avg:98.39ms
step:958/1770 train_time:94262ms step_avg:98.39ms
step:959/1770 train_time:94364ms step_avg:98.40ms
step:960/1770 train_time:94466ms step_avg:98.40ms
step:961/1770 train_time:94569ms step_avg:98.41ms
step:962/1770 train_time:94674ms step_avg:98.41ms
step:963/1770 train_time:94777ms step_avg:98.42ms
step:964/1770 train_time:94879ms step_avg:98.42ms
step:965/1770 train_time:94980ms step_avg:98.43ms
step:966/1770 train_time:95082ms step_avg:98.43ms
step:967/1770 train_time:95183ms step_avg:98.43ms
step:968/1770 train_time:95286ms step_avg:98.44ms
step:969/1770 train_time:95390ms step_avg:98.44ms
step:970/1770 train_time:95492ms step_avg:98.45ms
step:971/1770 train_time:95594ms step_avg:98.45ms
step:972/1770 train_time:95697ms step_avg:98.45ms
step:973/1770 train_time:95799ms step_avg:98.46ms
step:974/1770 train_time:95901ms step_avg:98.46ms
step:975/1770 train_time:96002ms step_avg:98.46ms
step:976/1770 train_time:96103ms step_avg:98.47ms
step:977/1770 train_time:96205ms step_avg:98.47ms
step:978/1770 train_time:96308ms step_avg:98.47ms
step:979/1770 train_time:96410ms step_avg:98.48ms
step:980/1770 train_time:96513ms step_avg:98.48ms
step:981/1770 train_time:96616ms step_avg:98.49ms
step:982/1770 train_time:96717ms step_avg:98.49ms
step:983/1770 train_time:96819ms step_avg:98.49ms
step:984/1770 train_time:96922ms step_avg:98.50ms
step:985/1770 train_time:97023ms step_avg:98.50ms
step:986/1770 train_time:97123ms step_avg:98.50ms
step:987/1770 train_time:97225ms step_avg:98.51ms
step:988/1770 train_time:97328ms step_avg:98.51ms
step:989/1770 train_time:97431ms step_avg:98.51ms
step:990/1770 train_time:97533ms step_avg:98.52ms
step:991/1770 train_time:97636ms step_avg:98.52ms
step:992/1770 train_time:97738ms step_avg:98.53ms
step:993/1770 train_time:97840ms step_avg:98.53ms
step:994/1770 train_time:97941ms step_avg:98.53ms
step:995/1770 train_time:98044ms step_avg:98.54ms
step:996/1770 train_time:98146ms step_avg:98.54ms
step:997/1770 train_time:98248ms step_avg:98.54ms
step:998/1770 train_time:98350ms step_avg:98.55ms
step:999/1770 train_time:98452ms step_avg:98.55ms
step:1000/1770 train_time:98555ms step_avg:98.56ms
step:1000/1770 val_loss:3.5175 train_time:98656ms step_avg:98.66ms
step:1001/1770 train_time:98674ms step_avg:98.58ms
step:1002/1770 train_time:98765ms step_avg:98.57ms
step:1003/1770 train_time:98867ms step_avg:98.57ms
step:1004/1770 train_time:98969ms step_avg:98.57ms
step:1005/1770 train_time:99069ms step_avg:98.58ms
step:1006/1770 train_time:99170ms step_avg:98.58ms
step:1007/1770 train_time:99272ms step_avg:98.58ms
step:1008/1770 train_time:99374ms step_avg:98.59ms
step:1009/1770 train_time:99475ms step_avg:98.59ms
step:1010/1770 train_time:99578ms step_avg:98.59ms
step:1011/1770 train_time:99683ms step_avg:98.60ms
step:1012/1770 train_time:99787ms step_avg:98.60ms
step:1013/1770 train_time:99889ms step_avg:98.61ms
step:1014/1770 train_time:99990ms step_avg:98.61ms
step:1015/1770 train_time:100091ms step_avg:98.61ms
step:1016/1770 train_time:100193ms step_avg:98.61ms
step:1017/1770 train_time:100295ms step_avg:98.62ms
step:1018/1770 train_time:100397ms step_avg:98.62ms
step:1019/1770 train_time:100499ms step_avg:98.63ms
step:1020/1770 train_time:100602ms step_avg:98.63ms
step:1021/1770 train_time:100705ms step_avg:98.63ms
step:1022/1770 train_time:100808ms step_avg:98.64ms
step:1023/1770 train_time:100910ms step_avg:98.64ms
step:1024/1770 train_time:101011ms step_avg:98.64ms
step:1025/1770 train_time:101113ms step_avg:98.65ms
step:1026/1770 train_time:101215ms step_avg:98.65ms
step:1027/1770 train_time:101318ms step_avg:98.65ms
step:1028/1770 train_time:101421ms step_avg:98.66ms
step:1029/1770 train_time:101523ms step_avg:98.66ms
step:1030/1770 train_time:101624ms step_avg:98.66ms
step:1031/1770 train_time:101727ms step_avg:98.67ms
step:1032/1770 train_time:101829ms step_avg:98.67ms
step:1033/1770 train_time:101932ms step_avg:98.68ms
step:1034/1770 train_time:102034ms step_avg:98.68ms
step:1035/1770 train_time:102137ms step_avg:98.68ms
step:1036/1770 train_time:102238ms step_avg:98.69ms
step:1037/1770 train_time:102340ms step_avg:98.69ms
step:1038/1770 train_time:102441ms step_avg:98.69ms
step:1039/1770 train_time:102543ms step_avg:98.69ms
step:1040/1770 train_time:102644ms step_avg:98.70ms
step:1041/1770 train_time:102746ms step_avg:98.70ms
step:1042/1770 train_time:102848ms step_avg:98.70ms
step:1043/1770 train_time:102951ms step_avg:98.71ms
step:1044/1770 train_time:103053ms step_avg:98.71ms
step:1045/1770 train_time:103156ms step_avg:98.71ms
step:1046/1770 train_time:103260ms step_avg:98.72ms
step:1047/1770 train_time:103361ms step_avg:98.72ms
step:1048/1770 train_time:103463ms step_avg:98.72ms
step:1049/1770 train_time:103565ms step_avg:98.73ms
step:1050/1770 train_time:103666ms step_avg:98.73ms
step:1051/1770 train_time:103770ms step_avg:98.73ms
step:1052/1770 train_time:103874ms step_avg:98.74ms
step:1053/1770 train_time:103976ms step_avg:98.74ms
step:1054/1770 train_time:104079ms step_avg:98.75ms
step:1055/1770 train_time:104182ms step_avg:98.75ms
step:1056/1770 train_time:104283ms step_avg:98.75ms
step:1057/1770 train_time:104385ms step_avg:98.76ms
step:1058/1770 train_time:104487ms step_avg:98.76ms
step:1059/1770 train_time:104589ms step_avg:98.76ms
step:1060/1770 train_time:104691ms step_avg:98.76ms
step:1061/1770 train_time:104793ms step_avg:98.77ms
step:1062/1770 train_time:104896ms step_avg:98.77ms
step:1063/1770 train_time:104999ms step_avg:98.78ms
step:1064/1770 train_time:105101ms step_avg:98.78ms
step:1065/1770 train_time:105203ms step_avg:98.78ms
step:1066/1770 train_time:105304ms step_avg:98.78ms
step:1067/1770 train_time:105407ms step_avg:98.79ms
step:1068/1770 train_time:105509ms step_avg:98.79ms
step:1069/1770 train_time:105611ms step_avg:98.79ms
step:1070/1770 train_time:105714ms step_avg:98.80ms
step:1071/1770 train_time:105818ms step_avg:98.80ms
step:1072/1770 train_time:105920ms step_avg:98.81ms
step:1073/1770 train_time:106022ms step_avg:98.81ms
step:1074/1770 train_time:106123ms step_avg:98.81ms
step:1075/1770 train_time:106226ms step_avg:98.82ms
step:1076/1770 train_time:106328ms step_avg:98.82ms
step:1077/1770 train_time:106430ms step_avg:98.82ms
step:1078/1770 train_time:106533ms step_avg:98.82ms
step:1079/1770 train_time:106636ms step_avg:98.83ms
step:1080/1770 train_time:106739ms step_avg:98.83ms
step:1081/1770 train_time:106841ms step_avg:98.84ms
step:1082/1770 train_time:106943ms step_avg:98.84ms
step:1083/1770 train_time:107044ms step_avg:98.84ms
step:1084/1770 train_time:107147ms step_avg:98.84ms
step:1085/1770 train_time:107250ms step_avg:98.85ms
step:1086/1770 train_time:107352ms step_avg:98.85ms
step:1087/1770 train_time:107455ms step_avg:98.85ms
step:1088/1770 train_time:107556ms step_avg:98.86ms
step:1089/1770 train_time:107659ms step_avg:98.86ms
step:1090/1770 train_time:107762ms step_avg:98.86ms
step:1091/1770 train_time:107863ms step_avg:98.87ms
step:1092/1770 train_time:107965ms step_avg:98.87ms
step:1093/1770 train_time:108067ms step_avg:98.87ms
step:1094/1770 train_time:108169ms step_avg:98.87ms
step:1095/1770 train_time:108272ms step_avg:98.88ms
step:1096/1770 train_time:108375ms step_avg:98.88ms
step:1097/1770 train_time:108478ms step_avg:98.89ms
step:1098/1770 train_time:108580ms step_avg:98.89ms
step:1099/1770 train_time:108682ms step_avg:98.89ms
step:1100/1770 train_time:108784ms step_avg:98.89ms
step:1101/1770 train_time:108886ms step_avg:98.90ms
step:1102/1770 train_time:108988ms step_avg:98.90ms
step:1103/1770 train_time:109091ms step_avg:98.90ms
step:1104/1770 train_time:109194ms step_avg:98.91ms
step:1105/1770 train_time:109297ms step_avg:98.91ms
step:1106/1770 train_time:109398ms step_avg:98.91ms
step:1107/1770 train_time:109501ms step_avg:98.92ms
step:1108/1770 train_time:109602ms step_avg:98.92ms
step:1109/1770 train_time:109704ms step_avg:98.92ms
step:1110/1770 train_time:109807ms step_avg:98.92ms
step:1111/1770 train_time:109910ms step_avg:98.93ms
step:1112/1770 train_time:110013ms step_avg:98.93ms
step:1113/1770 train_time:110116ms step_avg:98.94ms
step:1114/1770 train_time:110218ms step_avg:98.94ms
step:1115/1770 train_time:110320ms step_avg:98.94ms
step:1116/1770 train_time:110422ms step_avg:98.94ms
step:1117/1770 train_time:110524ms step_avg:98.95ms
step:1118/1770 train_time:110625ms step_avg:98.95ms
step:1119/1770 train_time:110729ms step_avg:98.95ms
step:1120/1770 train_time:110831ms step_avg:98.96ms
step:1121/1770 train_time:110932ms step_avg:98.96ms
step:1122/1770 train_time:111035ms step_avg:98.96ms
step:1123/1770 train_time:111138ms step_avg:98.97ms
step:1124/1770 train_time:111240ms step_avg:98.97ms
step:1125/1770 train_time:111342ms step_avg:98.97ms
step:1125/1770 val_loss:3.4775 train_time:111443ms step_avg:99.06ms
step:1126/1770 train_time:111461ms step_avg:98.99ms
step:1127/1770 train_time:111555ms step_avg:98.98ms
step:1128/1770 train_time:111658ms step_avg:98.99ms
step:1129/1770 train_time:111760ms step_avg:98.99ms
step:1130/1770 train_time:111862ms step_avg:98.99ms
step:1131/1770 train_time:111963ms step_avg:98.99ms
step:1132/1770 train_time:112064ms step_avg:99.00ms
step:1133/1770 train_time:112165ms step_avg:99.00ms
step:1134/1770 train_time:112266ms step_avg:99.00ms
step:1135/1770 train_time:112368ms step_avg:99.00ms
step:1136/1770 train_time:112475ms step_avg:99.01ms
step:1137/1770 train_time:112581ms step_avg:99.02ms
step:1138/1770 train_time:112684ms step_avg:99.02ms
step:1139/1770 train_time:112787ms step_avg:99.02ms
step:1140/1770 train_time:112889ms step_avg:99.03ms
step:1141/1770 train_time:112991ms step_avg:99.03ms
step:1142/1770 train_time:113092ms step_avg:99.03ms
step:1143/1770 train_time:113194ms step_avg:99.03ms
step:1144/1770 train_time:113296ms step_avg:99.03ms
step:1145/1770 train_time:113399ms step_avg:99.04ms
step:1146/1770 train_time:113503ms step_avg:99.04ms
step:1147/1770 train_time:113606ms step_avg:99.05ms
step:1148/1770 train_time:113709ms step_avg:99.05ms
step:1149/1770 train_time:113811ms step_avg:99.05ms
step:1150/1770 train_time:113913ms step_avg:99.05ms
step:1151/1770 train_time:114015ms step_avg:99.06ms
step:1152/1770 train_time:114116ms step_avg:99.06ms
step:1153/1770 train_time:114218ms step_avg:99.06ms
step:1154/1770 train_time:114320ms step_avg:99.06ms
step:1155/1770 train_time:114422ms step_avg:99.07ms
step:1156/1770 train_time:114524ms step_avg:99.07ms
step:1157/1770 train_time:114628ms step_avg:99.07ms
step:1158/1770 train_time:114731ms step_avg:99.08ms
step:1159/1770 train_time:114833ms step_avg:99.08ms
step:1160/1770 train_time:114935ms step_avg:99.08ms
step:1161/1770 train_time:115037ms step_avg:99.08ms
step:1162/1770 train_time:115139ms step_avg:99.09ms
step:1163/1770 train_time:115241ms step_avg:99.09ms
step:1164/1770 train_time:115343ms step_avg:99.09ms
step:1165/1770 train_time:115445ms step_avg:99.09ms
step:1166/1770 train_time:115547ms step_avg:99.10ms
step:1167/1770 train_time:115649ms step_avg:99.10ms
step:1168/1770 train_time:115752ms step_avg:99.10ms
step:1169/1770 train_time:115854ms step_avg:99.11ms
step:1170/1770 train_time:115958ms step_avg:99.11ms
step:1171/1770 train_time:116061ms step_avg:99.11ms
step:1172/1770 train_time:116163ms step_avg:99.12ms
step:1173/1770 train_time:116264ms step_avg:99.12ms
step:1174/1770 train_time:116366ms step_avg:99.12ms
step:1175/1770 train_time:116468ms step_avg:99.12ms
step:1176/1770 train_time:116570ms step_avg:99.12ms
step:1177/1770 train_time:116673ms step_avg:99.13ms
step:1178/1770 train_time:116775ms step_avg:99.13ms
step:1179/1770 train_time:116878ms step_avg:99.13ms
step:1180/1770 train_time:116980ms step_avg:99.14ms
step:1181/1770 train_time:117083ms step_avg:99.14ms
step:1182/1770 train_time:117185ms step_avg:99.14ms
step:1183/1770 train_time:117288ms step_avg:99.14ms
step:1184/1770 train_time:117392ms step_avg:99.15ms
step:1185/1770 train_time:117495ms step_avg:99.15ms
step:1186/1770 train_time:117599ms step_avg:99.16ms
step:1187/1770 train_time:117704ms step_avg:99.16ms
step:1188/1770 train_time:117807ms step_avg:99.16ms
step:1189/1770 train_time:117911ms step_avg:99.17ms
step:1190/1770 train_time:118015ms step_avg:99.17ms
step:1191/1770 train_time:118119ms step_avg:99.18ms
step:1192/1770 train_time:118223ms step_avg:99.18ms
step:1193/1770 train_time:118327ms step_avg:99.18ms
step:1194/1770 train_time:118429ms step_avg:99.19ms
step:1195/1770 train_time:118532ms step_avg:99.19ms
step:1196/1770 train_time:118637ms step_avg:99.19ms
step:1197/1770 train_time:118741ms step_avg:99.20ms
step:1198/1770 train_time:118844ms step_avg:99.20ms
step:1199/1770 train_time:118947ms step_avg:99.21ms
step:1200/1770 train_time:119051ms step_avg:99.21ms
step:1201/1770 train_time:119155ms step_avg:99.21ms
step:1202/1770 train_time:119259ms step_avg:99.22ms
step:1203/1770 train_time:119362ms step_avg:99.22ms
step:1204/1770 train_time:119465ms step_avg:99.22ms
step:1205/1770 train_time:119568ms step_avg:99.23ms
step:1206/1770 train_time:119674ms step_avg:99.23ms
step:1207/1770 train_time:119777ms step_avg:99.24ms
step:1208/1770 train_time:119881ms step_avg:99.24ms
step:1209/1770 train_time:119984ms step_avg:99.24ms
step:1210/1770 train_time:120086ms step_avg:99.24ms
step:1211/1770 train_time:120189ms step_avg:99.25ms
step:1212/1770 train_time:120295ms step_avg:99.25ms
step:1213/1770 train_time:120398ms step_avg:99.26ms
step:1214/1770 train_time:120503ms step_avg:99.26ms
step:1215/1770 train_time:120606ms step_avg:99.26ms
step:1216/1770 train_time:120710ms step_avg:99.27ms
step:1217/1770 train_time:120814ms step_avg:99.27ms
step:1218/1770 train_time:120917ms step_avg:99.28ms
step:1219/1770 train_time:121022ms step_avg:99.28ms
step:1220/1770 train_time:121125ms step_avg:99.28ms
step:1221/1770 train_time:121228ms step_avg:99.29ms
step:1222/1770 train_time:121332ms step_avg:99.29ms
step:1223/1770 train_time:121436ms step_avg:99.29ms
step:1224/1770 train_time:121540ms step_avg:99.30ms
step:1225/1770 train_time:121645ms step_avg:99.30ms
step:1226/1770 train_time:121747ms step_avg:99.30ms
step:1227/1770 train_time:121852ms step_avg:99.31ms
step:1228/1770 train_time:121957ms step_avg:99.31ms
step:1229/1770 train_time:122063ms step_avg:99.32ms
step:1230/1770 train_time:122166ms step_avg:99.32ms
step:1231/1770 train_time:122269ms step_avg:99.32ms
step:1232/1770 train_time:122372ms step_avg:99.33ms
step:1233/1770 train_time:122475ms step_avg:99.33ms
step:1234/1770 train_time:122581ms step_avg:99.34ms
step:1235/1770 train_time:122684ms step_avg:99.34ms
step:1236/1770 train_time:122788ms step_avg:99.34ms
step:1237/1770 train_time:122891ms step_avg:99.35ms
step:1238/1770 train_time:122996ms step_avg:99.35ms
step:1239/1770 train_time:123099ms step_avg:99.35ms
step:1240/1770 train_time:123203ms step_avg:99.36ms
step:1241/1770 train_time:123307ms step_avg:99.36ms
step:1242/1770 train_time:123410ms step_avg:99.36ms
step:1243/1770 train_time:123514ms step_avg:99.37ms
step:1244/1770 train_time:123617ms step_avg:99.37ms
step:1245/1770 train_time:123722ms step_avg:99.37ms
step:1246/1770 train_time:123827ms step_avg:99.38ms
step:1247/1770 train_time:123930ms step_avg:99.38ms
step:1248/1770 train_time:124033ms step_avg:99.39ms
step:1249/1770 train_time:124136ms step_avg:99.39ms
step:1250/1770 train_time:124240ms step_avg:99.39ms
step:1250/1770 val_loss:3.4279 train_time:124344ms step_avg:99.48ms
step:1251/1770 train_time:124364ms step_avg:99.41ms
step:1252/1770 train_time:124457ms step_avg:99.41ms
step:1253/1770 train_time:124561ms step_avg:99.41ms
step:1254/1770 train_time:124664ms step_avg:99.41ms
step:1255/1770 train_time:124769ms step_avg:99.42ms
step:1256/1770 train_time:124872ms step_avg:99.42ms
step:1257/1770 train_time:124974ms step_avg:99.42ms
step:1258/1770 train_time:125077ms step_avg:99.43ms
step:1259/1770 train_time:125181ms step_avg:99.43ms
step:1260/1770 train_time:125287ms step_avg:99.43ms
step:1261/1770 train_time:125393ms step_avg:99.44ms
step:1262/1770 train_time:125498ms step_avg:99.44ms
step:1263/1770 train_time:125602ms step_avg:99.45ms
step:1264/1770 train_time:125706ms step_avg:99.45ms
step:1265/1770 train_time:125809ms step_avg:99.45ms
step:1266/1770 train_time:125912ms step_avg:99.46ms
step:1267/1770 train_time:126015ms step_avg:99.46ms
step:1268/1770 train_time:126117ms step_avg:99.46ms
step:1269/1770 train_time:126221ms step_avg:99.47ms
step:1270/1770 train_time:126327ms step_avg:99.47ms
step:1271/1770 train_time:126433ms step_avg:99.48ms
step:1272/1770 train_time:126537ms step_avg:99.48ms
step:1273/1770 train_time:126640ms step_avg:99.48ms
step:1274/1770 train_time:126744ms step_avg:99.48ms
step:1275/1770 train_time:126847ms step_avg:99.49ms
step:1276/1770 train_time:126951ms step_avg:99.49ms
step:1277/1770 train_time:127055ms step_avg:99.49ms
step:1278/1770 train_time:127157ms step_avg:99.50ms
step:1279/1770 train_time:127262ms step_avg:99.50ms
step:1280/1770 train_time:127366ms step_avg:99.51ms
step:1281/1770 train_time:127471ms step_avg:99.51ms
step:1282/1770 train_time:127575ms step_avg:99.51ms
step:1283/1770 train_time:127679ms step_avg:99.52ms
step:1284/1770 train_time:127782ms step_avg:99.52ms
step:1285/1770 train_time:127886ms step_avg:99.52ms
step:1286/1770 train_time:127990ms step_avg:99.53ms
step:1287/1770 train_time:128094ms step_avg:99.53ms
step:1288/1770 train_time:128196ms step_avg:99.53ms
step:1289/1770 train_time:128301ms step_avg:99.54ms
step:1290/1770 train_time:128406ms step_avg:99.54ms
step:1291/1770 train_time:128510ms step_avg:99.54ms
step:1292/1770 train_time:128613ms step_avg:99.55ms
step:1293/1770 train_time:128716ms step_avg:99.55ms
step:1294/1770 train_time:128819ms step_avg:99.55ms
step:1295/1770 train_time:128923ms step_avg:99.55ms
step:1296/1770 train_time:129026ms step_avg:99.56ms
step:1297/1770 train_time:129132ms step_avg:99.56ms
step:1298/1770 train_time:129234ms step_avg:99.56ms
step:1299/1770 train_time:129338ms step_avg:99.57ms
step:1300/1770 train_time:129443ms step_avg:99.57ms
step:1301/1770 train_time:129546ms step_avg:99.57ms
step:1302/1770 train_time:129649ms step_avg:99.58ms
step:1303/1770 train_time:129752ms step_avg:99.58ms
step:1304/1770 train_time:129855ms step_avg:99.58ms
step:1305/1770 train_time:129958ms step_avg:99.58ms
step:1306/1770 train_time:130062ms step_avg:99.59ms
step:1307/1770 train_time:130166ms step_avg:99.59ms
step:1308/1770 train_time:130269ms step_avg:99.59ms
step:1309/1770 train_time:130373ms step_avg:99.60ms
step:1310/1770 train_time:130476ms step_avg:99.60ms
step:1311/1770 train_time:130580ms step_avg:99.60ms
step:1312/1770 train_time:130684ms step_avg:99.61ms
step:1313/1770 train_time:130787ms step_avg:99.61ms
step:1314/1770 train_time:130892ms step_avg:99.61ms
step:1315/1770 train_time:130994ms step_avg:99.62ms
step:1316/1770 train_time:131097ms step_avg:99.62ms
step:1317/1770 train_time:131201ms step_avg:99.62ms
step:1318/1770 train_time:131306ms step_avg:99.63ms
step:1319/1770 train_time:131410ms step_avg:99.63ms
step:1320/1770 train_time:131513ms step_avg:99.63ms
step:1321/1770 train_time:131616ms step_avg:99.63ms
step:1322/1770 train_time:131720ms step_avg:99.64ms
step:1323/1770 train_time:131826ms step_avg:99.64ms
step:1324/1770 train_time:131930ms step_avg:99.65ms
step:1325/1770 train_time:132034ms step_avg:99.65ms
step:1326/1770 train_time:132137ms step_avg:99.65ms
step:1327/1770 train_time:132242ms step_avg:99.65ms
step:1328/1770 train_time:132347ms step_avg:99.66ms
step:1329/1770 train_time:132451ms step_avg:99.66ms
step:1330/1770 train_time:132554ms step_avg:99.66ms
step:1331/1770 train_time:132657ms step_avg:99.67ms
step:1332/1770 train_time:132760ms step_avg:99.67ms
step:1333/1770 train_time:132864ms step_avg:99.67ms
step:1334/1770 train_time:132967ms step_avg:99.68ms
step:1335/1770 train_time:133072ms step_avg:99.68ms
step:1336/1770 train_time:133176ms step_avg:99.68ms
step:1337/1770 train_time:133279ms step_avg:99.69ms
step:1338/1770 train_time:133383ms step_avg:99.69ms
step:1339/1770 train_time:133487ms step_avg:99.69ms
step:1340/1770 train_time:133591ms step_avg:99.69ms
step:1341/1770 train_time:133696ms step_avg:99.70ms
step:1342/1770 train_time:133800ms step_avg:99.70ms
step:1343/1770 train_time:133905ms step_avg:99.71ms
step:1344/1770 train_time:134009ms step_avg:99.71ms
step:1345/1770 train_time:134113ms step_avg:99.71ms
step:1346/1770 train_time:134216ms step_avg:99.71ms
step:1347/1770 train_time:134320ms step_avg:99.72ms
step:1348/1770 train_time:134426ms step_avg:99.72ms
step:1349/1770 train_time:134530ms step_avg:99.73ms
step:1350/1770 train_time:134634ms step_avg:99.73ms
step:1351/1770 train_time:134737ms step_avg:99.73ms
step:1352/1770 train_time:134841ms step_avg:99.73ms
step:1353/1770 train_time:134946ms step_avg:99.74ms
step:1354/1770 train_time:135050ms step_avg:99.74ms
step:1355/1770 train_time:135153ms step_avg:99.74ms
step:1356/1770 train_time:135257ms step_avg:99.75ms
step:1357/1770 train_time:135361ms step_avg:99.75ms
step:1358/1770 train_time:135465ms step_avg:99.75ms
step:1359/1770 train_time:135570ms step_avg:99.76ms
step:1360/1770 train_time:135673ms step_avg:99.76ms
step:1361/1770 train_time:135777ms step_avg:99.76ms
step:1362/1770 train_time:135882ms step_avg:99.77ms
step:1363/1770 train_time:135988ms step_avg:99.77ms
step:1364/1770 train_time:136092ms step_avg:99.77ms
step:1365/1770 train_time:136196ms step_avg:99.78ms
step:1366/1770 train_time:136299ms step_avg:99.78ms
step:1367/1770 train_time:136404ms step_avg:99.78ms
step:1368/1770 train_time:136507ms step_avg:99.79ms
step:1369/1770 train_time:136612ms step_avg:99.79ms
step:1370/1770 train_time:136718ms step_avg:99.79ms
step:1371/1770 train_time:136820ms step_avg:99.80ms
step:1372/1770 train_time:136923ms step_avg:99.80ms
step:1373/1770 train_time:137027ms step_avg:99.80ms
step:1374/1770 train_time:137132ms step_avg:99.81ms
step:1375/1770 train_time:137237ms step_avg:99.81ms
step:1375/1770 val_loss:3.3849 train_time:137340ms step_avg:99.88ms
step:1376/1770 train_time:137358ms step_avg:99.82ms
step:1377/1770 train_time:137453ms step_avg:99.82ms
step:1378/1770 train_time:137556ms step_avg:99.82ms
step:1379/1770 train_time:137660ms step_avg:99.83ms
step:1380/1770 train_time:137763ms step_avg:99.83ms
step:1381/1770 train_time:137866ms step_avg:99.83ms
step:1382/1770 train_time:137969ms step_avg:99.83ms
step:1383/1770 train_time:138072ms step_avg:99.83ms
step:1384/1770 train_time:138175ms step_avg:99.84ms
step:1385/1770 train_time:138279ms step_avg:99.84ms
step:1386/1770 train_time:138387ms step_avg:99.85ms
step:1387/1770 train_time:138493ms step_avg:99.85ms
step:1388/1770 train_time:138596ms step_avg:99.85ms
step:1389/1770 train_time:138701ms step_avg:99.86ms
step:1390/1770 train_time:138805ms step_avg:99.86ms
step:1391/1770 train_time:138908ms step_avg:99.86ms
step:1392/1770 train_time:139010ms step_avg:99.86ms
step:1393/1770 train_time:139113ms step_avg:99.87ms
step:1394/1770 train_time:139216ms step_avg:99.87ms
step:1395/1770 train_time:139323ms step_avg:99.87ms
step:1396/1770 train_time:139427ms step_avg:99.88ms
step:1397/1770 train_time:139532ms step_avg:99.88ms
step:1398/1770 train_time:139635ms step_avg:99.88ms
step:1399/1770 train_time:139740ms step_avg:99.89ms
step:1400/1770 train_time:139844ms step_avg:99.89ms
step:1401/1770 train_time:139946ms step_avg:99.89ms
step:1402/1770 train_time:140050ms step_avg:99.89ms
step:1403/1770 train_time:140153ms step_avg:99.90ms
step:1404/1770 train_time:140257ms step_avg:99.90ms
step:1405/1770 train_time:140361ms step_avg:99.90ms
step:1406/1770 train_time:140466ms step_avg:99.90ms
step:1407/1770 train_time:140569ms step_avg:99.91ms
step:1408/1770 train_time:140673ms step_avg:99.91ms
step:1409/1770 train_time:140776ms step_avg:99.91ms
step:1410/1770 train_time:140881ms step_avg:99.92ms
step:1411/1770 train_time:140984ms step_avg:99.92ms
step:1412/1770 train_time:141087ms step_avg:99.92ms
step:1413/1770 train_time:141191ms step_avg:99.92ms
step:1414/1770 train_time:141295ms step_avg:99.93ms
step:1415/1770 train_time:141399ms step_avg:99.93ms
step:1416/1770 train_time:141505ms step_avg:99.93ms
step:1417/1770 train_time:141609ms step_avg:99.94ms
step:1418/1770 train_time:141715ms step_avg:99.94ms
step:1419/1770 train_time:141819ms step_avg:99.94ms
step:1420/1770 train_time:141922ms step_avg:99.95ms
step:1421/1770 train_time:142026ms step_avg:99.95ms
step:1422/1770 train_time:142128ms step_avg:99.95ms
step:1423/1770 train_time:142231ms step_avg:99.95ms
step:1424/1770 train_time:142335ms step_avg:99.95ms
step:1425/1770 train_time:142439ms step_avg:99.96ms
step:1426/1770 train_time:142544ms step_avg:99.96ms
step:1427/1770 train_time:142647ms step_avg:99.96ms
step:1428/1770 train_time:142753ms step_avg:99.97ms
step:1429/1770 train_time:142858ms step_avg:99.97ms
step:1430/1770 train_time:142962ms step_avg:99.97ms
step:1431/1770 train_time:143066ms step_avg:99.98ms
step:1432/1770 train_time:143169ms step_avg:99.98ms
step:1433/1770 train_time:143272ms step_avg:99.98ms
step:1434/1770 train_time:143376ms step_avg:99.98ms
step:1435/1770 train_time:143480ms step_avg:99.99ms
step:1436/1770 train_time:143585ms step_avg:99.99ms
step:1437/1770 train_time:143688ms step_avg:99.99ms
step:1438/1770 train_time:143793ms step_avg:99.99ms
step:1439/1770 train_time:143896ms step_avg:100.00ms
step:1440/1770 train_time:144001ms step_avg:100.00ms
step:1441/1770 train_time:144106ms step_avg:100.00ms
step:1442/1770 train_time:144208ms step_avg:100.01ms
step:1443/1770 train_time:144311ms step_avg:100.01ms
step:1444/1770 train_time:144415ms step_avg:100.01ms
step:1445/1770 train_time:144521ms step_avg:100.01ms
step:1446/1770 train_time:144627ms step_avg:100.02ms
step:1447/1770 train_time:144732ms step_avg:100.02ms
step:1448/1770 train_time:144837ms step_avg:100.03ms
step:1449/1770 train_time:144944ms step_avg:100.03ms
step:1450/1770 train_time:145048ms step_avg:100.03ms
step:1451/1770 train_time:145152ms step_avg:100.04ms
step:1452/1770 train_time:145256ms step_avg:100.04ms
step:1453/1770 train_time:145361ms step_avg:100.04ms
step:1454/1770 train_time:145466ms step_avg:100.05ms
step:1455/1770 train_time:145571ms step_avg:100.05ms
step:1456/1770 train_time:145675ms step_avg:100.05ms
step:1457/1770 train_time:145780ms step_avg:100.06ms
step:1458/1770 train_time:145885ms step_avg:100.06ms
step:1459/1770 train_time:145990ms step_avg:100.06ms
step:1460/1770 train_time:146094ms step_avg:100.06ms
step:1461/1770 train_time:146198ms step_avg:100.07ms
step:1462/1770 train_time:146303ms step_avg:100.07ms
step:1463/1770 train_time:146407ms step_avg:100.07ms
step:1464/1770 train_time:146513ms step_avg:100.08ms
step:1465/1770 train_time:146618ms step_avg:100.08ms
step:1466/1770 train_time:146725ms step_avg:100.09ms
step:1467/1770 train_time:146830ms step_avg:100.09ms
step:1468/1770 train_time:146935ms step_avg:100.09ms
step:1469/1770 train_time:147040ms step_avg:100.10ms
step:1470/1770 train_time:147144ms step_avg:100.10ms
step:1471/1770 train_time:147249ms step_avg:100.10ms
step:1472/1770 train_time:147353ms step_avg:100.10ms
step:1473/1770 train_time:147459ms step_avg:100.11ms
step:1474/1770 train_time:147563ms step_avg:100.11ms
step:1475/1770 train_time:147668ms step_avg:100.11ms
step:1476/1770 train_time:147773ms step_avg:100.12ms
step:1477/1770 train_time:147879ms step_avg:100.12ms
step:1478/1770 train_time:147986ms step_avg:100.13ms
step:1479/1770 train_time:148091ms step_avg:100.13ms
step:1480/1770 train_time:148196ms step_avg:100.13ms
step:1481/1770 train_time:148304ms step_avg:100.14ms
step:1482/1770 train_time:148407ms step_avg:100.14ms
step:1483/1770 train_time:148512ms step_avg:100.14ms
step:1484/1770 train_time:148617ms step_avg:100.15ms
step:1485/1770 train_time:148722ms step_avg:100.15ms
step:1486/1770 train_time:148827ms step_avg:100.15ms
step:1487/1770 train_time:148931ms step_avg:100.16ms
step:1488/1770 train_time:149037ms step_avg:100.16ms
step:1489/1770 train_time:149142ms step_avg:100.16ms
step:1490/1770 train_time:149248ms step_avg:100.17ms
step:1491/1770 train_time:149352ms step_avg:100.17ms
step:1492/1770 train_time:149459ms step_avg:100.17ms
step:1493/1770 train_time:149567ms step_avg:100.18ms
step:1494/1770 train_time:149673ms step_avg:100.18ms
step:1495/1770 train_time:149777ms step_avg:100.19ms
step:1496/1770 train_time:149884ms step_avg:100.19ms
step:1497/1770 train_time:149988ms step_avg:100.19ms
step:1498/1770 train_time:150093ms step_avg:100.20ms
step:1499/1770 train_time:150197ms step_avg:100.20ms
step:1500/1770 train_time:150301ms step_avg:100.20ms
step:1500/1770 val_loss:3.3470 train_time:150405ms step_avg:100.27ms
step:1501/1770 train_time:150424ms step_avg:100.22ms
step:1502/1770 train_time:150517ms step_avg:100.21ms
step:1503/1770 train_time:150621ms step_avg:100.21ms
step:1504/1770 train_time:150726ms step_avg:100.22ms
step:1505/1770 train_time:150832ms step_avg:100.22ms
step:1506/1770 train_time:150937ms step_avg:100.22ms
step:1507/1770 train_time:151042ms step_avg:100.23ms
step:1508/1770 train_time:151147ms step_avg:100.23ms
step:1509/1770 train_time:151251ms step_avg:100.23ms
step:1510/1770 train_time:151359ms step_avg:100.24ms
step:1511/1770 train_time:151468ms step_avg:100.24ms
step:1512/1770 train_time:151572ms step_avg:100.25ms
step:1513/1770 train_time:151677ms step_avg:100.25ms
step:1514/1770 train_time:151781ms step_avg:100.25ms
step:1515/1770 train_time:151887ms step_avg:100.26ms
step:1516/1770 train_time:151991ms step_avg:100.26ms
step:1517/1770 train_time:152096ms step_avg:100.26ms
step:1518/1770 train_time:152201ms step_avg:100.26ms
step:1519/1770 train_time:152306ms step_avg:100.27ms
step:1520/1770 train_time:152415ms step_avg:100.27ms
step:1521/1770 train_time:152521ms step_avg:100.28ms
step:1522/1770 train_time:152627ms step_avg:100.28ms
step:1523/1770 train_time:152732ms step_avg:100.28ms
step:1524/1770 train_time:152835ms step_avg:100.29ms
step:1525/1770 train_time:152940ms step_avg:100.29ms
step:1526/1770 train_time:153046ms step_avg:100.29ms
step:1527/1770 train_time:153149ms step_avg:100.29ms
step:1528/1770 train_time:153255ms step_avg:100.30ms
step:1529/1770 train_time:153360ms step_avg:100.30ms
step:1530/1770 train_time:153466ms step_avg:100.30ms
step:1531/1770 train_time:153570ms step_avg:100.31ms
step:1532/1770 train_time:153675ms step_avg:100.31ms
step:1533/1770 train_time:153781ms step_avg:100.31ms
step:1534/1770 train_time:153887ms step_avg:100.32ms
step:1535/1770 train_time:153991ms step_avg:100.32ms
step:1536/1770 train_time:154096ms step_avg:100.32ms
step:1537/1770 train_time:154200ms step_avg:100.33ms
step:1538/1770 train_time:154305ms step_avg:100.33ms
step:1539/1770 train_time:154410ms step_avg:100.33ms
step:1540/1770 train_time:154516ms step_avg:100.34ms
step:1541/1770 train_time:154622ms step_avg:100.34ms
step:1542/1770 train_time:154726ms step_avg:100.34ms
step:1543/1770 train_time:154830ms step_avg:100.34ms
step:1544/1770 train_time:154938ms step_avg:100.35ms
step:1545/1770 train_time:155042ms step_avg:100.35ms
step:1546/1770 train_time:155147ms step_avg:100.35ms
step:1547/1770 train_time:155251ms step_avg:100.36ms
step:1548/1770 train_time:155356ms step_avg:100.36ms
step:1549/1770 train_time:155461ms step_avg:100.36ms
step:1550/1770 train_time:155566ms step_avg:100.37ms
step:1551/1770 train_time:155671ms step_avg:100.37ms
step:1552/1770 train_time:155776ms step_avg:100.37ms
step:1553/1770 train_time:155881ms step_avg:100.37ms
step:1554/1770 train_time:155986ms step_avg:100.38ms
step:1555/1770 train_time:156093ms step_avg:100.38ms
step:1556/1770 train_time:156196ms step_avg:100.38ms
step:1557/1770 train_time:156300ms step_avg:100.39ms
step:1558/1770 train_time:156404ms step_avg:100.39ms
step:1559/1770 train_time:156509ms step_avg:100.39ms
step:1560/1770 train_time:156614ms step_avg:100.39ms
step:1561/1770 train_time:156722ms step_avg:100.40ms
step:1562/1770 train_time:156826ms step_avg:100.40ms
step:1563/1770 train_time:156930ms step_avg:100.40ms
step:1564/1770 train_time:157034ms step_avg:100.41ms
step:1565/1770 train_time:157139ms step_avg:100.41ms
step:1566/1770 train_time:157244ms step_avg:100.41ms
step:1567/1770 train_time:157348ms step_avg:100.41ms
step:1568/1770 train_time:157452ms step_avg:100.42ms
step:1569/1770 train_time:157559ms step_avg:100.42ms
step:1570/1770 train_time:157664ms step_avg:100.42ms
step:1571/1770 train_time:157769ms step_avg:100.43ms
step:1572/1770 train_time:157875ms step_avg:100.43ms
step:1573/1770 train_time:157981ms step_avg:100.43ms
step:1574/1770 train_time:158086ms step_avg:100.44ms
step:1575/1770 train_time:158189ms step_avg:100.44ms
step:1576/1770 train_time:158294ms step_avg:100.44ms
step:1577/1770 train_time:158402ms step_avg:100.44ms
step:1578/1770 train_time:158508ms step_avg:100.45ms
step:1579/1770 train_time:158612ms step_avg:100.45ms
step:1580/1770 train_time:158717ms step_avg:100.45ms
step:1581/1770 train_time:158823ms step_avg:100.46ms
step:1582/1770 train_time:158927ms step_avg:100.46ms
step:1583/1770 train_time:159032ms step_avg:100.46ms
step:1584/1770 train_time:159138ms step_avg:100.47ms
step:1585/1770 train_time:159243ms step_avg:100.47ms
step:1586/1770 train_time:159352ms step_avg:100.47ms
step:1587/1770 train_time:159457ms step_avg:100.48ms
step:1588/1770 train_time:159561ms step_avg:100.48ms
step:1589/1770 train_time:159667ms step_avg:100.48ms
step:1590/1770 train_time:159771ms step_avg:100.49ms
step:1591/1770 train_time:159876ms step_avg:100.49ms
step:1592/1770 train_time:159982ms step_avg:100.49ms
step:1593/1770 train_time:160088ms step_avg:100.49ms
step:1594/1770 train_time:160192ms step_avg:100.50ms
step:1595/1770 train_time:160297ms step_avg:100.50ms
step:1596/1770 train_time:160405ms step_avg:100.50ms
step:1597/1770 train_time:160509ms step_avg:100.51ms
step:1598/1770 train_time:160615ms step_avg:100.51ms
step:1599/1770 train_time:160719ms step_avg:100.51ms
step:1600/1770 train_time:160826ms step_avg:100.52ms
step:1601/1770 train_time:160931ms step_avg:100.52ms
step:1602/1770 train_time:161037ms step_avg:100.52ms
step:1603/1770 train_time:161142ms step_avg:100.53ms
step:1604/1770 train_time:161247ms step_avg:100.53ms
step:1605/1770 train_time:161351ms step_avg:100.53ms
step:1606/1770 train_time:161456ms step_avg:100.53ms
step:1607/1770 train_time:161565ms step_avg:100.54ms
step:1608/1770 train_time:161670ms step_avg:100.54ms
step:1609/1770 train_time:161777ms step_avg:100.54ms
step:1610/1770 train_time:161883ms step_avg:100.55ms
step:1611/1770 train_time:161988ms step_avg:100.55ms
step:1612/1770 train_time:162093ms step_avg:100.55ms
step:1613/1770 train_time:162198ms step_avg:100.56ms
step:1614/1770 train_time:162303ms step_avg:100.56ms
step:1615/1770 train_time:162409ms step_avg:100.56ms
step:1616/1770 train_time:162513ms step_avg:100.57ms
step:1617/1770 train_time:162620ms step_avg:100.57ms
step:1618/1770 train_time:162725ms step_avg:100.57ms
step:1619/1770 train_time:162832ms step_avg:100.58ms
step:1620/1770 train_time:162938ms step_avg:100.58ms
step:1621/1770 train_time:163042ms step_avg:100.58ms
step:1622/1770 train_time:163147ms step_avg:100.58ms
step:1623/1770 train_time:163253ms step_avg:100.59ms
step:1624/1770 train_time:163357ms step_avg:100.59ms
step:1625/1770 train_time:163463ms step_avg:100.59ms
step:1625/1770 val_loss:3.3129 train_time:163567ms step_avg:100.66ms
step:1626/1770 train_time:163587ms step_avg:100.61ms
step:1627/1770 train_time:163678ms step_avg:100.60ms
step:1628/1770 train_time:163782ms step_avg:100.60ms
step:1629/1770 train_time:163887ms step_avg:100.61ms
step:1630/1770 train_time:163991ms step_avg:100.61ms
step:1631/1770 train_time:164095ms step_avg:100.61ms
step:1632/1770 train_time:164198ms step_avg:100.61ms
step:1633/1770 train_time:164302ms step_avg:100.61ms
step:1634/1770 train_time:164407ms step_avg:100.62ms
step:1635/1770 train_time:164513ms step_avg:100.62ms
step:1636/1770 train_time:164621ms step_avg:100.62ms
step:1637/1770 train_time:164728ms step_avg:100.63ms
step:1638/1770 train_time:164833ms step_avg:100.63ms
step:1639/1770 train_time:164938ms step_avg:100.63ms
step:1640/1770 train_time:165043ms step_avg:100.64ms
step:1641/1770 train_time:165147ms step_avg:100.64ms
step:1642/1770 train_time:165250ms step_avg:100.64ms
step:1643/1770 train_time:165353ms step_avg:100.64ms
step:1644/1770 train_time:165459ms step_avg:100.64ms
step:1645/1770 train_time:165564ms step_avg:100.65ms
step:1646/1770 train_time:165670ms step_avg:100.65ms
step:1647/1770 train_time:165776ms step_avg:100.65ms
step:1648/1770 train_time:165881ms step_avg:100.66ms
step:1649/1770 train_time:165988ms step_avg:100.66ms
step:1650/1770 train_time:166093ms step_avg:100.66ms
step:1651/1770 train_time:166196ms step_avg:100.66ms
step:1652/1770 train_time:166300ms step_avg:100.67ms
step:1653/1770 train_time:166405ms step_avg:100.67ms
step:1654/1770 train_time:166514ms step_avg:100.67ms
step:1655/1770 train_time:166620ms step_avg:100.68ms
step:1656/1770 train_time:166727ms step_avg:100.68ms
step:1657/1770 train_time:166833ms step_avg:100.68ms
step:1658/1770 train_time:166939ms step_avg:100.69ms
step:1659/1770 train_time:167045ms step_avg:100.69ms
step:1660/1770 train_time:167150ms step_avg:100.69ms
step:1661/1770 train_time:167255ms step_avg:100.70ms
step:1662/1770 train_time:167360ms step_avg:100.70ms
step:1663/1770 train_time:167463ms step_avg:100.70ms
step:1664/1770 train_time:167568ms step_avg:100.70ms
step:1665/1770 train_time:167673ms step_avg:100.70ms
step:1666/1770 train_time:167779ms step_avg:100.71ms
step:1667/1770 train_time:167884ms step_avg:100.71ms
step:1668/1770 train_time:167990ms step_avg:100.71ms
step:1669/1770 train_time:168094ms step_avg:100.72ms
step:1670/1770 train_time:168198ms step_avg:100.72ms
step:1671/1770 train_time:168304ms step_avg:100.72ms
step:1672/1770 train_time:168410ms step_avg:100.72ms
step:1673/1770 train_time:168515ms step_avg:100.73ms
step:1674/1770 train_time:168618ms step_avg:100.73ms
step:1675/1770 train_time:168723ms step_avg:100.73ms
step:1676/1770 train_time:168829ms step_avg:100.73ms
step:1677/1770 train_time:168938ms step_avg:100.74ms
step:1678/1770 train_time:169042ms step_avg:100.74ms
step:1679/1770 train_time:169149ms step_avg:100.74ms
step:1680/1770 train_time:169252ms step_avg:100.75ms
step:1681/1770 train_time:169358ms step_avg:100.75ms
step:1682/1770 train_time:169464ms step_avg:100.75ms
step:1683/1770 train_time:169569ms step_avg:100.75ms
step:1684/1770 train_time:169673ms step_avg:100.76ms
step:1685/1770 train_time:169777ms step_avg:100.76ms
step:1686/1770 train_time:169883ms step_avg:100.76ms
step:1687/1770 train_time:169990ms step_avg:100.76ms
step:1688/1770 train_time:170096ms step_avg:100.77ms
step:1689/1770 train_time:170200ms step_avg:100.77ms
step:1690/1770 train_time:170305ms step_avg:100.77ms
step:1691/1770 train_time:170412ms step_avg:100.78ms
step:1692/1770 train_time:170517ms step_avg:100.78ms
step:1693/1770 train_time:170623ms step_avg:100.78ms
step:1694/1770 train_time:170729ms step_avg:100.78ms
step:1695/1770 train_time:170834ms step_avg:100.79ms
step:1696/1770 train_time:170939ms step_avg:100.79ms
step:1697/1770 train_time:171046ms step_avg:100.79ms
step:1698/1770 train_time:171150ms step_avg:100.80ms
step:1699/1770 train_time:171255ms step_avg:100.80ms
step:1700/1770 train_time:171359ms step_avg:100.80ms
step:1701/1770 train_time:171466ms step_avg:100.80ms
step:1702/1770 train_time:171571ms step_avg:100.81ms
step:1703/1770 train_time:171674ms step_avg:100.81ms
step:1704/1770 train_time:171779ms step_avg:100.81ms
step:1705/1770 train_time:171884ms step_avg:100.81ms
step:1706/1770 train_time:171990ms step_avg:100.81ms
step:1707/1770 train_time:172097ms step_avg:100.82ms
step:1708/1770 train_time:172203ms step_avg:100.82ms
step:1709/1770 train_time:172308ms step_avg:100.82ms
step:1710/1770 train_time:172418ms step_avg:100.83ms
step:1711/1770 train_time:172525ms step_avg:100.83ms
step:1712/1770 train_time:172631ms step_avg:100.84ms
step:1713/1770 train_time:172735ms step_avg:100.84ms
step:1714/1770 train_time:172841ms step_avg:100.84ms
step:1715/1770 train_time:172946ms step_avg:100.84ms
step:1716/1770 train_time:173053ms step_avg:100.85ms
step:1717/1770 train_time:173158ms step_avg:100.85ms
step:1718/1770 train_time:173265ms step_avg:100.85ms
step:1719/1770 train_time:173372ms step_avg:100.86ms
step:1720/1770 train_time:173478ms step_avg:100.86ms
step:1721/1770 train_time:173584ms step_avg:100.86ms
step:1722/1770 train_time:173691ms step_avg:100.87ms
step:1723/1770 train_time:173798ms step_avg:100.87ms
step:1724/1770 train_time:173905ms step_avg:100.87ms
step:1725/1770 train_time:174012ms step_avg:100.88ms
step:1726/1770 train_time:174118ms step_avg:100.88ms
step:1727/1770 train_time:174224ms step_avg:100.88ms
step:1728/1770 train_time:174331ms step_avg:100.89ms
step:1729/1770 train_time:174438ms step_avg:100.89ms
step:1730/1770 train_time:174544ms step_avg:100.89ms
step:1731/1770 train_time:174651ms step_avg:100.90ms
step:1732/1770 train_time:174756ms step_avg:100.90ms
step:1733/1770 train_time:174864ms step_avg:100.90ms
step:1734/1770 train_time:174969ms step_avg:100.90ms
step:1735/1770 train_time:175074ms step_avg:100.91ms
step:1736/1770 train_time:175180ms step_avg:100.91ms
step:1737/1770 train_time:175286ms step_avg:100.91ms
step:1738/1770 train_time:175393ms step_avg:100.92ms
step:1739/1770 train_time:175500ms step_avg:100.92ms
step:1740/1770 train_time:175605ms step_avg:100.92ms
step:1741/1770 train_time:175713ms step_avg:100.93ms
step:1742/1770 train_time:175820ms step_avg:100.93ms
step:1743/1770 train_time:175927ms step_avg:100.93ms
step:1744/1770 train_time:176032ms step_avg:100.94ms
step:1745/1770 train_time:176139ms step_avg:100.94ms
step:1746/1770 train_time:176246ms step_avg:100.94ms
step:1747/1770 train_time:176350ms step_avg:100.94ms
step:1748/1770 train_time:176457ms step_avg:100.95ms
step:1749/1770 train_time:176563ms step_avg:100.95ms
step:1750/1770 train_time:176671ms step_avg:100.96ms
step:1750/1770 val_loss:3.2860 train_time:176776ms step_avg:101.01ms
step:1751/1770 train_time:176795ms step_avg:100.97ms
step:1752/1770 train_time:176887ms step_avg:100.96ms
step:1753/1770 train_time:176993ms step_avg:100.97ms
step:1754/1770 train_time:177098ms step_avg:100.97ms
step:1755/1770 train_time:177204ms step_avg:100.97ms
step:1756/1770 train_time:177309ms step_avg:100.97ms
step:1757/1770 train_time:177415ms step_avg:100.98ms
step:1758/1770 train_time:177520ms step_avg:100.98ms
step:1759/1770 train_time:177628ms step_avg:100.98ms
step:1760/1770 train_time:177735ms step_avg:100.99ms
step:1761/1770 train_time:177846ms step_avg:100.99ms
step:1762/1770 train_time:177955ms step_avg:101.00ms
step:1763/1770 train_time:178060ms step_avg:101.00ms
step:1764/1770 train_time:178166ms step_avg:101.00ms
step:1765/1770 train_time:178272ms step_avg:101.00ms
step:1766/1770 train_time:178382ms step_avg:101.01ms
step:1767/1770 train_time:178486ms step_avg:101.01ms
step:1768/1770 train_time:178591ms step_avg:101.01ms
step:1769/1770 train_time:178698ms step_avg:101.02ms
step:1770/1770 train_time:178805ms step_avg:101.02ms
step:1770/1770 val_loss:3.2828 train_time:178912ms step_avg:101.08ms
peak memory allocated: 30725 MiB reserved: 46474 MiB
